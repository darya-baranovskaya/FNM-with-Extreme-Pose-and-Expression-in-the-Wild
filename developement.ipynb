{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_bias = False\n",
    "front_path = './images/fig1'\n",
    "profile_path = '.images/fig2'\n",
    "lambda_l1 = 0.001, #'weight of the loss for L1 texture loss') # 0.001\n",
    "lambda_fea=100 #'weight of the loss for face model feature loss')\n",
    "lambda_reg= 1e-5# 'weight of the loss for L2 regularitaion loss')\n",
    "lambda_gan= 1# 'weight of the loss for gan loss')\n",
    "lambda_gp=10# 'weight of the loss for gradient penalty on parameter of D')\n",
    "\n",
    "# For training\n",
    "dataset_size=  1000# 'dataset path')  # casia_aligned_250_250_jpg\n",
    "profile_list=''# 'train profile list')\n",
    "front_path=''#front data path')\n",
    "front_list=''# 'train front list')\n",
    "test_path=''# 'front data path')\n",
    "is_train=True# 'train or test')\n",
    "is_finetune= False# 'finetune') # False, True\n",
    "face_mode='resnet50.npy'# 'face model path')\n",
    "checkpoint='checkpoint/fnm'# 'checkpoint directory')\n",
    "summary_dir= 'log/fnm'# 'logs directory')\n",
    "checkpoint_ft='checkpoint/fnm/ck-09'#'finetune or test checkpoint path')\n",
    "batch_size= 64# 'batch size')#was 16\n",
    "epoc=10 # 'epoch')\n",
    "critic= 1 #'number of D training times')\n",
    "save_freq= 1000 # 'the frequency of saving model')\n",
    "lr=1e-4# 'base learning rate')\n",
    "beta1=0. # 'beta1 momentum term of adam')\n",
    "beta2=0.9 # 'beta2 momentum term of adam')\n",
    "stddev= 0.02 # 'stddev for W initializer')\n",
    "use_bias=False # 'whether to use bias')\n",
    "results='results/fnm' # 'path for saving results') #\n",
    "############################\n",
    "batch_size = 4\n",
    "############################\n",
    "#   environment setting    #\n",
    "############################\n",
    "device_id='3,4'# 'device id')\n",
    "ori_height=224 # 'original height of profile images')\n",
    "ori_width=224 # 'original width of profile images')\n",
    "height= 224 #'height of images') # do not modified\n",
    "width= 224 # 'width of images') # do not modified\n",
    "CHANNEL=3 # 'channel of images')\n",
    "num_threads=8 # 'number of threads of enqueueing examples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50 model trained on VGGFace2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Resnet50_ft_dag import resnet50_ft_dag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Batch_norm(nn.Module):\n",
    "#     def __init__(self, in_channels, epsilon=1e-5, momentum = 0.9, is_train = False):\n",
    "#         super(Batch_norm, self).__init__()\n",
    "#         # self.mean  = mosv_dict['mean']\n",
    "#         self.norm = nn.BatchNorm2d(in_channels, eps = epsilon,  momentum = momentum, track_running_stats = is_train)\n",
    "#         #self.scale = mosv_dict['scale']\n",
    "#         #self.variance = mosv_dict['variance']\n",
    "#         #self.epsilon = 1e-5\n",
    "#     def forward(self, x):\n",
    "#         norm = self.norm(x)\n",
    "#         return norm\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Batch_norm(in_channels, epsilon=1e-5, momentum = 0.9, is_train = False):\n",
    "    return nn.BatchNorm2d(in_channels, eps = epsilon,  momentum = momentum, track_running_stats = is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM = Batch_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options: conv2d, res_block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_conv_pad(input_size, output_size, filter_size = 3, stride = 2):\n",
    "    return max(0, floor((stride * (output_size - 1) + filter_size - input_size)/2 + 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_deconv_pad(input_size, output_size, filter_size = 3, stride = 2):\n",
    "    return max(0, floor((stride * (input_size - 1) + filter_size - output_size)/2 + 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d( in_channels , out_channels , kernel_size = 3 , strides = 1  , \n",
    "           padding  =  None,  pad_input = None,\n",
    "           bias = use_bias,dilation_rate = 1, activation = None):\n",
    "    layers = []\n",
    "    if padding == None:\n",
    "        if pad_input is not None:\n",
    "            padding = calc_conv_pad(pad_input , 2*pad_input, kernel_size , strides)\n",
    "        else:\n",
    "            padding = 0\n",
    "    conv = nn.Conv2d( in_channels , out_channels , bias=bias, kernel_size = kernel_size, stride=strides, padding=padding, dilation=dilation_rate )\n",
    "    layers.append( conv )\n",
    "    if activation is not None:\n",
    "        layers.append( activation )\n",
    "    return nn.Sequential( *layers )\n",
    "\n",
    "def deconv2d(in_channels , out_channels , kernel_size = 3 , strides = 1  , \n",
    "             padding  = None, pad_input = None,\n",
    "             bias= use_bias, dilation_rate = 1,\n",
    "            activation = None):\n",
    "    if padding == None:\n",
    "        if pad_input is not None:\n",
    "            padding = calc_deconv_pad(pad_input , 2*pad_input, kernel_size , strides)\n",
    "        else:\n",
    "            padding = 0\n",
    "    layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size =  kernel_size, stride=strides, \n",
    "                      padding=padding, bias=bias)]\n",
    "    if activation is not None:\n",
    "        layers.append( activation )\n",
    "    return nn.Sequential( *layers )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class res_block(nn.Module):\n",
    "    def __init__(self,  in_channels , \n",
    "                 out_channels , kernel_size = 3, \n",
    "                 stride = 1  , padding  =  None, pad_input = None,\n",
    "                 bias = use_bias,  norm = NORM, activation2 = nn.ReLU, activation = None,):\n",
    "        super(res_block, self).__init__()\n",
    "        if padding == None:\n",
    "            if pad_input is not None:\n",
    "                padding = calc_conv_pad(pad_input , pad_input, kernel_size , stride)\n",
    "            else: \n",
    "                padding = 0\n",
    "        self.out_channels = out_channels\n",
    "        if activation is not None:\n",
    "            self.activation = activation(out_channels)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "        self.activation2 = activation2(out_channels)\n",
    "        convs = [conv2d(in_channels , out_channels , kernel_size , stride , padding, bias, activation = self.activation), \n",
    "             nn.BatchNorm2d( in_channels ), \n",
    "             nn.ReLU(out_channels), \n",
    "             conv2d(out_channels, out_channels, kernel_size, stride, padding, bias, activation = self.activation),\n",
    "             norm(in_channels)]\n",
    "        self.layers = nn.Sequential(*convs)\n",
    "        #независимо от передаваемого параметра NORM норма всегда Batch_norm\n",
    "    def forward(self, x):\n",
    "        return self.activation2(self.layers(x) + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "src_dir = \"your/source/dir\"\n",
    "dst_dir = \"your/destination/dir\"\n",
    "profile_set = set()\n",
    "profile_list = []\n",
    "front_set = set()\n",
    "front_list = []\n",
    "\n",
    "def relocate_data(src_dir, src_dir_front, src_dir_profile):\n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.png\")):\n",
    "        tmp = jpgfile.split('\\\\')[-1].split('.')[0].split('_')\n",
    "        if (len(tmp) > 6):\n",
    "            if tmp[0] not in profile_set:\n",
    "                shutil.copy(jpgfile, src_dir_profile)\n",
    "                profile_list.append(jpgfile)\n",
    "                profile_set.add(tmp[0])\n",
    "        else:\n",
    "            if tmp[0] not in front_set:\n",
    "                shutil.copy(jpgfile, src_dir_front)\n",
    "                front_list.append(jpgfile)\n",
    "                front_set.add(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = \"C:/Users/d.baranovska/Downloads/15\"\n",
    "src_dir_front = \"./TPdata/train_data/front\"\n",
    "src_dir_profile = \"./TPdata/train_data/profile\"\n",
    "relocate_data(src_dir, src_dir_front, src_dir_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loadData(object):\n",
    "#   \"\"\"Class for loading data.\n",
    "  \n",
    "#   This is a class for loading data (e.g. image) to model. Train image \n",
    "#   and test image can be obtained by function \"get_train\" and \n",
    "#   function \"get_test_batch\" respectively.\n",
    "  \n",
    "#   Args:\n",
    "#     batch_size (int): size of every train batch\n",
    "#     train_shuffle (bool): whether to shuffle train set.\n",
    "    \n",
    "#   \"\"\"\n",
    "  \n",
    "    def __init__(self, batch_size = BS, train_shuffle = True):\n",
    "        self.batch_size = batch_size\n",
    "        self.profile = np.loadtxt(cfg.profile_list, dtype='string', delimiter=',')\n",
    "        self.front = np.loadtxt(cfg.front_list, dtype='string', delimiter=',')\n",
    "    \n",
    "        if(train_shuffle): \n",
    "            np.random.shuffle(self.profile)\n",
    "            np.random.shuffle(self.front)\n",
    "             \n",
    "        self.test_list = np.loadtxt(cfg.test_list, dtype='string',delimiter=',') #\n",
    "        self.test_index = 0\n",
    "    \n",
    "    # Crop Box: left, upper, right, lower\n",
    "        self.crop_box = [(cfg.ori_width - cfg.width) / 2, (cfg.ori_height - cfg.height) / 2,\n",
    "            (cfg.ori_width + cfg.width) / 2, (cfg.ori_height + cfg.height) / 2]     \n",
    "        assert Image.open(os.path.join(cfg.profile_path, self.profile[0])).size == \\\n",
    "         (cfg.ori_width, cfg.ori_height)\n",
    "  \n",
    "    def get_train(self):\n",
    "#     \"\"\"Get train images\n",
    "    \n",
    "#     Train images will be horizontal-flipped, center-cropped and adjust brightness randomly.\n",
    "    \n",
    "#     return:\n",
    "#       profile (tf.tensor): profile of identity A\n",
    "#       front (tf.tensor): front face of identity B\n",
    "#     \"\"\"\n",
    "    \n",
    "        with tf.name_scope('data_feed'):\n",
    "            profile_list = [os.path.join(cfg.profile_path,img) for img in self.profile]\n",
    "            front_list = [os.path.join(cfg.front_path,img) for img in self.front]\n",
    "            profile_files = tf.train.string_input_producer(profile_list, shuffle=False) #\n",
    "            front_files = tf.train.string_input_producer(front_list, shuffle=False) #\n",
    "      \n",
    "            _, profile_value = tf.WholeFileReader().read(profile_files)\n",
    "            profile_value = tf.image.decode_jpeg(profile_value, channels=cfg.channel)\n",
    "            profile_value = tf.cast(profile_value, tf.float32)\n",
    "            _, front_value = tf.WholeFileReader().read(front_files)\n",
    "            front_value = tf.image.decode_jpeg(front_value, channels=cfg.channel)\n",
    "            front_value = tf.cast(front_value, tf.float32)\n",
    "      \n",
    "      \n",
    "      # Flip, crop and adjust brightness of  image\n",
    "            profile_value = tf.image.random_brightness(profile_value, max_delta=20.)\n",
    "            profile_value = tf.clip_by_value(profile_value, clip_value_min=0., clip_value_max=255.)\n",
    "            profile_value = tf.image.random_flip_left_right(profile_value)\n",
    "            profile_value = tf.random_crop(profile_value, [cfg.height, cfg.width, cfg.channel])\n",
    "      \n",
    "            front_value = tf.image.random_brightness(front_value, max_delta=20.)\n",
    "            front_value = tf.clip_by_value(front_value, clip_value_min=0., clip_value_max=255.)\n",
    "      # Args: [image, offset_height, offset_width, target_height, target_width]\n",
    "      # front_value = tf.image.resize_images(front_value, [cfg.height, cfg.width])\n",
    "            front_value = tf.image.resize_images(front_value, [cfg.height, cfg.width])\n",
    "                            \n",
    "            profile,front = tf.train.shuffle_batch([profile_value, front_value],\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_threads=8,\n",
    "                          capacity=32 * self.batch_size,\n",
    "                          min_after_dequeue=self.batch_size * 16,\n",
    "                          allow_smaller_final_batch=False)\n",
    "        return profile, front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, img_list):\n",
    "        super(type(self), self).__init__()\n",
    "        self.img_list = img.list\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    def __getitem__(self, idx):\n",
    "        #229_01_01_200_08_cropped_test.png\n",
    "        batch = {}\n",
    "        img_name = self.img_list[idx]\n",
    "        img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.loadtxt()\n",
    "assert Image.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r\"./data/train_data\"\n",
    "DATA_PATH_Train_front=r'./data/train_data/front'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image size: even though image sizes are bigger than 96, we use this to speed up training\n",
    "SIZE_H = SIZE_W = 96\n",
    "\n",
    "# Number of classes in the dataset\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Epochs: number of passes over the training data, we use it this small to reduce training babysitting time\n",
    "EPOCH_NUM = 30\n",
    "\n",
    "# Batch size: for batch gradient descent optimization, usually selected as 2**K elements\n",
    "\n",
    "\n",
    "# Images mean and std channelwise\n",
    "image_mean = [0.485, 0.456, 0.406]\n",
    "image_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Last layer (embeddings) size for CNN models\n",
    "EMBEDDING_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((SIZE_H, SIZE_W)),        # scaling images to fixed size\n",
    "    transforms.ToTensor(),                      # converting to tensors\n",
    "    transforms.Normalize(image_mean, image_std) # normalize image data per-channel\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "transforms.ToTensor(), transforms.Normalize([0.5], [0.5]), transforms.CenterCrop(255)])\n",
    "\n",
    "train_dataset_front = datasets.ImageFolder(DATA_PATH_Train_front, transform=transformer)\n",
    "train_dataset_profile = datasets.ImageFolder(os.path.join(DATA_PATH, 'profile'),  transform=transformer)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader_front = torch.utils.data.DataLoader(dataset=train_dataset_front, batch_size=batch_size, shuffle=True)\n",
    "train_loader_profile = torch.utils.data.DataLoader(dataset=train_dataset_profile, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './data/test_data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-e5b05c32d136>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./data/test_data/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    207\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m     91\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[0;32m     92\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[1;34m(self, dir)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# Faster and available in Python 3.5 and above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './data/test_data/'"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.ImageFolder(root='./data/test_data/', transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dset.ImageFolder(root=dataroot,\n",
    "#                            transform=transforms.Compose([\n",
    "#                                transforms.Resize(image_size),\n",
    "#                                transforms.CenterCrop(image_size),\n",
    "#                                transforms.ToTensor(),\n",
    "#                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]))\n",
    "# # Create the dataloader\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "#                                          shuffle=True, num_workers=workers)\n",
    "\n",
    "# # Decide which device we want to run on\n",
    "# device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# # Plot some training images\n",
    "# real_batch = next(iter(dataloader))\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.axis(\"off\")\n",
    "# plt.title(\"Training Images\")\n",
    "# plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "f7_shape = [7, 7, 2048]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### second variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self): #, profile, front, train):\n",
    "        super(Generator, self).__init__() \n",
    "#         self.face_model = Resnet50_ft_dag()\n",
    "#         self.feature_p = self.face_model.forward(profile)\n",
    "#         self.feature_f = self.face_model.forward(front)\n",
    "#         self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
    "#         self.is_train = train\n",
    "        \n",
    "        self.conv1 = nn.Sequential(conv2d(2048, 512, kernel_size=1, strides = 1), NORM(512, is_train), nn.ReLU())\n",
    "    \n",
    "        self.res1_1 = res_block(512, 512, norm = NORM, pad_input = 7)\n",
    "        self.res1_2 = res_block(512, 512, norm = NORM, pad_input = 7)\n",
    "        self.res1_3 = res_block(512, 512, norm = NORM, pad_input = 7)\n",
    "        self.res1_4 = res_block(512, 512, norm = NORM, pad_input = 7)\n",
    "        self.dconv2 = nn.Sequential(deconv2d(512, 256, kernel_size=4, strides = 2, pad_input = 7), NORM(256), nn.ReLU())\n",
    "        self.res2 = res_block(256, 256, norm = NORM, pad_input = 14)\n",
    "        self.dconv3 = nn.Sequential(deconv2d(256, 128, kernel_size=4, strides = 2, pad_input = 14) , NORM(128), nn.ReLU())\n",
    "        self.res3 = res_block(128, 128, norm = NORM, pad_input = 28)\n",
    "        self.dconv4 = nn.Sequential(deconv2d(128, 64, kernel_size=4, strides = 2, pad_input = 28), NORM(64), nn.ReLU())\n",
    "        self.res4 = res_block(64, 64, norm = NORM, pad_input = 56)\n",
    "        self.dconv5 = nn.Sequential(deconv2d(64, 32, kernel_size=4, strides = 2, pad_input = 56), NORM(32), nn.ReLU())\n",
    "        self.res5 = res_block(32, 32, norm = NORM, pad_input = 112)\n",
    "        self.dconv6 = nn.Sequential(deconv2d(32, 32, kernel_size=4, strides = 2, pad_input =112),  NORM(32), nn.ReLU())\n",
    "        self.res6 = res_block(32, 32, norm = NORM,  pad_input = 224)\n",
    "        self.gen = nn.Sequential(conv2d(32, 3,  kernel_size=1, strides = 1), nn.Tanh())\n",
    "        \n",
    "    def forward(self, feature):\n",
    "        feat7 = feature[0]\n",
    "        feat7 = self.conv1(feat7)\n",
    "        pool5 = feature[1]\n",
    "        res1_1 = self.res1_1(feat7)\n",
    "        res1_2 = self.res1_2(res1_1)\n",
    "        res1_3 = self.res1_3(res1_2)\n",
    "        res1_4 = self.res1_2(res1_3)\n",
    "        dconv2 = self.dconv2(res1_4)\n",
    "        res2 = self.res2(dconv2)\n",
    "        dconv3 = self.dconv3(res2)\n",
    "        res3 = self.res3(dconv3)\n",
    "        dconv4 = self.dconv4(res3)\n",
    "        res4 = self.res4(dconv4)\n",
    "        dconv5 = self.dconv5(res4)\n",
    "        res5 = self.res5(dconv5)\n",
    "        dconv6 = self.dconv6(res5)\n",
    "        res6 = self.res6(dconv6)\n",
    "        gen = self.gen(res6)\n",
    "        return (gen + 1)* 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        pad = 1\n",
    "        self.h0_0 = nn.Sequential(conv2d(3, 32, kernel_size=3, strides=2, padding = pad), nn.LeakyReLU())\n",
    "        self.h0_1 = nn.Sequential(conv2d(32, 64, kernel_size=3, strides=2, padding = pad), NORM(64), nn.LeakyReLU())\n",
    "        self.h0_2 = nn.Sequential(conv2d(64, 128, kernel_size=3, strides=2, padding = pad), NORM(128), nn.LeakyReLU())\n",
    "        self.h0_3 = nn.Sequential(conv2d(128, 256, kernel_size=3, strides=2, padding = pad), NORM(256), nn.LeakyReLU())\n",
    "        self.h0_4 = nn.Sequential(conv2d(256, 256, kernel_size=3, strides=2, padding = pad), NORM(256), nn.LeakyReLU())\n",
    "        self.h0_5 = nn.Linear(12544, 1)\n",
    "        \n",
    "        self.h1_0 = nn.Sequential(conv2d(3, 32, kernel_size=3, strides=2, padding = pad), nn.LeakyReLU())\n",
    "        self.h1_1 = nn.Sequential(conv2d(32, 64, kernel_size=3, strides=2, padding = pad), NORM(64), nn.LeakyReLU())\n",
    "        self.h1_2 = nn.Sequential(conv2d(64, 128, kernel_size=3, strides=2, padding = pad), NORM(128), nn.LeakyReLU())\n",
    "        self.h1_3 = nn.Sequential(conv2d(128, 256, kernel_size=3, strides=2, padding = pad), NORM(256), nn.LeakyReLU())\n",
    "        self.h1_4 = nn.Linear(6144, 1)\n",
    "        \n",
    "        self.h2_0 = nn.Sequential(conv2d(3, 32, kernel_size=3, strides=2, padding = pad), nn.LeakyReLU())\n",
    "        self.h2_1 = nn.Sequential(conv2d(32, 64, kernel_size=3, strides=2, padding = pad), NORM(64), nn.LeakyReLU())\n",
    "        self.h2_2 = nn.Sequential(conv2d(64, 128, kernel_size=3, strides=2, padding = pad), NORM(128), nn.LeakyReLU())\n",
    "        self.h2_3 = nn.Sequential(conv2d(128, 256, kernel_size=3, strides=2, padding = pad), NORM(256), nn.LeakyReLU())\n",
    "        #self.h2_4 = nn.Linear(self.h2_3.out_features, 1)\n",
    "        self.h2_4 = nn.Linear(3840, 1)\n",
    "        \n",
    "        self.h3_0 = nn.Sequential(conv2d(3, 32, kernel_size=3, strides=2, padding = pad), nn.LeakyReLU())\n",
    "        self.h3_1 = nn.Sequential(conv2d(32, 64, kernel_size=3, strides=2, padding = pad), NORM(64), nn.LeakyReLU())\n",
    "        self.h3_2 = nn.Sequential(conv2d(64, 128, kernel_size=3, strides=2, padding = pad), NORM(128), nn.LeakyReLU())\n",
    "        self.h3_3 = nn.Sequential(conv2d(128, 256, kernel_size=3, strides=2, padding = pad), NORM(256), nn.LeakyReLU())\n",
    "        self.h3_4 = nn.Linear(2560, 1)\n",
    "        \n",
    "        self.h4_0 = nn.Sequential(conv2d(3, 32, kernel_size=3, strides=2, padding = pad), nn.LeakyReLU())\n",
    "        self.h4_1 = nn.Sequential(conv2d(32, 64, kernel_size=3, strides=2, padding = pad), NORM(64), nn.LeakyReLU())\n",
    "        self.h4_2 = nn.Sequential(conv2d(64, 128, kernel_size=3, strides=2, padding = pad), NORM(128), nn.LeakyReLU())\n",
    "        self.h4_3 = nn.Sequential(conv2d(128, 256, kernel_size=3, strides=2, padding = pad), NORM(256), nn.LeakyReLU())\n",
    "        self.h4_4 = nn.Linear(16384, 1)\n",
    "    \n",
    "    \n",
    "    def forward(self, images):\n",
    "        eyes = images[0:BS,0:CHANNEL,64:100,50:174] #[BS,36,124,CHANNEL])\n",
    "        nose =  images[0:BS,0:CHANNEL,75:140,90:134]#tf.slice(images, [0,75,90,0], [BS,65,44,CHANNEL])\n",
    "        mouth = images[0:BS,0:CHANNEL,140:170,75:149]#tf.slice(images, [0,140,75,0], [BS,30,74,CHANNEL])\n",
    "        face = images[0:BS,0:CHANNEL,64:180,50:174]#[BS,116,124,CHANNEL])\n",
    "        h0_0 = self.h0_0(images)\n",
    "        h0_1 = self.h0_1(h0_0)\n",
    "        h0_2 = self.h0_2(h0_1)\n",
    "        h0_3 = self.h0_3(h0_2)\n",
    "        h0_4 = self.h0_4(h0_3)\n",
    "        h0_5 = self.h0_5(torch.reshape(h0_4, [BS, -1]))\n",
    "        \n",
    "        h1_0 = self.h1_0(eyes)\n",
    "        h1_1 = self.h1_1(h1_0)\n",
    "        h1_2 = self.h1_2(h1_1)\n",
    "        h1_3 = self.h1_3(h1_2)\n",
    "        h1_4 = self.h1_4(torch.reshape(h1_3, [BS, -1]))\n",
    "        \n",
    "        h2_0 = self.h2_0(nose)\n",
    "        h2_1 = self.h2_1(h2_0)\n",
    "        h2_2 = self.h2_2(h2_1)\n",
    "        h2_3 = self.h2_3(h2_2)\n",
    "        h2_4 = self.h2_4(torch.reshape(h2_3, [BS, -1]))\n",
    "        \n",
    "        h3_0 = self.h3_0(mouth)\n",
    "        h3_1 = self.h3_1(h3_0)\n",
    "        h3_2 = self.h3_2(h3_1)\n",
    "        h3_3 = self.h3_3(h3_2)\n",
    "        h3_4 = self.h3_4(torch.reshape(h3_3, [BS, -1]))\n",
    "        \n",
    "        h4_0 = self.h4_0(face)\n",
    "        h4_1 = self.h4_1(h4_0)\n",
    "        h4_2 = self.h4_2(h4_1)\n",
    "        h4_3 = self.h4_3(h4_2)\n",
    "        h4_4 = self.h4_4(torch.reshape(h4_3, [BS, -1]))\n",
    "        return h0_5, h1_4, h2_4, h3_4, h4_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#built network\n",
    "z_dim = 224*224\n",
    "mnist_dim = 224*224\n",
    "#mnist_dim = train_dataset_front.train_data.size(1) * train_dataset.train_data.size(2)\n",
    "\n",
    "G = Generator()\n",
    "D = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss() \n",
    "\n",
    "# optimizer\n",
    "lr = 0.0002 \n",
    "G_optimizer = optim.Adam(G.parameters(),betas=(beta1, beta2))\n",
    "D_optimizer = optim.Adam(D.parameters(), betas=(beta1, beta2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "Losses = []\n",
    "D_losses, G_losses = [], []\n",
    "D_finalLosses, G_finalLosses = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse(x, dim):\n",
    "    dim = x.dim() + dim if dim < 0 else dim\n",
    "    return x[tuple(slice(None, None) if i != dim\n",
    "             else torch.arange(x.size(i)-1, -1, -1).long()\n",
    "             for i in range(x.dim()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(profile, front):\n",
    "    #=======================Train the generator=======================#\n",
    "    feature_p = Model.forward(profile) # G_enc(x)\n",
    "    feature_f = Model.forward(front) # G_enc(y)\n",
    "    G.zero_grad()\n",
    "    gen_p = G.forward(feature_p) # ~x\n",
    "    gen_f = G.forward(feature_f) # ~y\n",
    "    feature_gen_p = Model.forward(gen_p) #G_enc(~x)\n",
    "    feature_gen_f = Model.forward(gen_f) #G_enc(~y)\n",
    "    \n",
    "    pool5_p_norm = feature_p[-1]/(torch.norm(feature_p[-1],dim = 1, keepdim = True) + epsilon)\n",
    "    pool5_f_norm = feature_f[-1]/(torch.norm(feature_f[-1],dim = 1, keepdim = True) + epsilon)\n",
    "    \n",
    "    pool5_gen_p_norm = feature_gen_p[-1]/(torch.norm(feature_gen_p[-1],dim = 1, keepdim = True) + epsilon)\n",
    "    pool5_gen_f_norm = feature_gen_f[-1]/(torch.norm(feature_gen_f[-1],dim = 1, keepdim = True) + epsilon)\n",
    "    \n",
    "     # 1. Frontalization Loss: L1-Norm\n",
    "    front_loss = torch.mean(torch.sum(torch.abs(front/255. - gen_f/255.), [1,2,3]))\n",
    "    \n",
    "    # 2. identity perseption loss l2-norm\n",
    "    feature_distance = 0.5*(1 - torch.sum(torch.mul(pool5_p_norm, pool5_gen_p_norm), [1])) + \\\n",
    "                                0.5*(1 - torch.sum(torch.mul(pool5_f_norm, pool5_gen_f_norm), [1]))\n",
    "    feature_loss = torch.mean(feature_distance)\n",
    "    Losses.append(feature_loss)\n",
    "    \n",
    "    #.\n",
    "    #trainable var\n",
    "    #all_vars = torch.autograd.Variables()\n",
    "    vars_gen = G.parameters(requires_grad= True)\n",
    "    vars_dis = D.parameters(requires_grad= True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 3. Regulation loss\n",
    "    loss = nn.MSELoss(lambda_reg)\n",
    "    reg_gen = loss(weights_list=[var for var in vars_gen]) # if 'kernel' in var.name\n",
    "    reg_dis = loss(weights_list=[var for var in vars_dis])# if 'kernel' in var.name\n",
    "    G_losses.append(reg_gen)\n",
    "    D_losses.append(reg_dis)\n",
    "    \n",
    "    \n",
    "    # 4. Adversarial Loss\n",
    "    d_loss = torch.mean(torch.add(self.df1)*0.5 + torch.add(self.df2)*0.5 - tf.add_n(self.dr)) / 5\n",
    "    self.g_loss = - tf.reduce_mean(torch.add(df1)*0.5 + tf.add(df2)*0.5) / 5\n",
    "    D_losses.append(self.d_loss)\n",
    "    G_losses.append(self.g_loss)\n",
    "    \n",
    "    # 5. Symmetric Loss - not applied\n",
    "    mirror_p = reverse(self.gen_p, dim=[2])\n",
    "    sym_loss = torch.mean(torch.sum(torchf.abs(mirror_p/225. - gen_p/255.), [1,2,3]))\n",
    "      \n",
    "     # 6. Drift Loss - not applied\n",
    "    self.drift_loss = 0\n",
    "    #torch.mean(torch.add(torch.square(df)) + torch.add(torch.square(dr))) / 10\n",
    "\n",
    "    Gen_loss =  reg_gen + lambda_l1 * front_loss + lambda_fea * feature_loss + g_loss/lambda_gan\n",
    "    Dis_loss = lambda_gan * d_loss + lambda_gp * gradient_penalty + 1/reg_dis\n",
    "    G_finalLosses.append(Gen_loss)\n",
    "    D_finalLosses.append(Dis_loss)\n",
    "    Gen_loss.backward()\n",
    "    Dis_loss.backward()\n",
    "    \n",
    "    G_optimizer.step()\n",
    "    D_optimizer.step()\n",
    "    return Gen_loss,  Dis_loss\n",
    "#     z = Variable(torch.randn(bs, z_dim).to(device))\n",
    "#     y = Variable(torch.ones(bs, 1).to(device))\n",
    "\n",
    "#     G_output = G(z)\n",
    "#     D_output = D(G_output)\n",
    "#     G_loss = criterion(D_output, y)\n",
    "\n",
    "#     # gradient backprop & optimize ONLY G's parameters\n",
    "#     G_loss.backward()\n",
    "#     G_optimizer.step()\n",
    "        \n",
    "#     return G_loss.data.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x17acbb77148>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path= \"./resnet50_ft_dag.pth\"\n",
    "Model = resnet50_ft_dag(weights_path=weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand([4, 3, 224, 224])\n",
    "t = Model(x)\n",
    "G = Generator()\n",
    "x_gen = G(t)\n",
    "print(x_gen.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1]) torch.Size([4, 1]) torch.Size([4, 1]) torch.Size([4, 1]) torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "D = Discriminator()\n",
    "x_dis = D(x_gen)\n",
    "print(x_dis[0].size(), x_dis[1].size(), x_dis[2].size(), x_dis[3].size(), x_dis[4].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "BS = batch_size\n",
    "Gen_loss, Dis_loss = loss(x, train_dataset_front[i])\n",
    "D_losses.append(Gen_loss)\n",
    "G_losses.append(Dis_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<PIL.Image.Image image mode=RGB size=224x224 at 0x17A83191788>) with an unsupported type (<class 'PIL.Image.Image'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_image_ops.py\u001b[0m in \u001b[0;36mdecode_jpeg\u001b[1;34m(contents, channels, ratio, fancy_upscaling, try_recover_truncated, acceptable_fraction, dct_method, name)\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[0mfancy_upscaling\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"try_recover_truncated\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtry_recover_truncated\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m         \"acceptable_fraction\", acceptable_fraction, \"dct_method\", dct_method)\n\u001b[0m\u001b[0;32m   1169\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-c0383277186e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmy_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_jpeg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_image_ops.py\u001b[0m in \u001b[0;36mdecode_jpeg\u001b[1;34m(contents, channels, ratio, fancy_upscaling, try_recover_truncated, acceptable_fraction, dct_method, name)\u001b[0m\n\u001b[0;32m   1175\u001b[0m             \u001b[0mtry_recover_truncated\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtry_recover_truncated\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0macceptable_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0macceptable_fraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdct_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdct_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             name=name, ctx=_ctx)\n\u001b[0m\u001b[0;32m   1178\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_image_ops.py\u001b[0m in \u001b[0;36mdecode_jpeg_eager_fallback\u001b[1;34m(contents, channels, ratio, fancy_upscaling, try_recover_truncated, acceptable_fraction, dct_method, name, ctx)\u001b[0m\n\u001b[0;32m   1252\u001b[0m     \u001b[0mdct_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m   \u001b[0mdct_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdct_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dct_method\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m   \u001b[0mcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m   _attrs = (\"channels\", channels, \"ratio\", ratio, \"fancy_upscaling\",\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[0;32m   1182\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[0;32m   1183\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[1;32m-> 1184\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1240\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m       as_ref=False)\n\u001b[0m\u001b[0;32m   1243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    284\u001b[0m                                          as_ref=False):\n\u001b[0;32m    285\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    225\u001b[0m   \"\"\"\n\u001b[0;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[1;32m--> 227\u001b[1;33m                         allow_broadcast=True)\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    233\u001b[0m   \u001b[0mctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (<PIL.Image.Image image mode=RGB size=224x224 at 0x17A83191788>) with an unsupported type (<class 'PIL.Image.Image'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "image = Image.open(\"./data/train_data/front/0001_01/0001_1.jpg\")\n",
    "image = image.resize([224, 224])\n",
    "\n",
    "my_img = tf.image.decode_jpeg(image, channels=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core._api.v2.train' has no attribute 'string_input_producer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-5221ff7ed9fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'./data/train_data/front/0001_01/0001_1.jpg'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfilepath_queue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_input_producer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWholeFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core._api.v2.train' has no attribute 'string_input_producer'"
     ]
    }
   ],
   "source": [
    "string = ['./data/train_data/front/0001_01/0001_1.jpg']\n",
    "filepath_queue = tf.train.string_input_producer(string)\n",
    "self.reader = tf.WholeFileReader()\n",
    "key, value = self.reader.read(filepath_queue)\n",
    "\n",
    "print(value)\n",
    "# Output: Tensor(\"ReaderRead:1\", shape=TensorShape([]), dtype=string)\n",
    "\n",
    "my_img = tf.image.decode_jpeg(value, channels=3)    \n",
    "KNOWN_HEIGHT = 28\n",
    "KNOWN_WIDTH = 28\n",
    "my_img.set_shape([KNOWN_HEIGHT, KNOWN_WIDTH, 3])\n",
    "\n",
    "print(my_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (2048x3x3). Calculated output size: (2048x-3x-3). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-d8bfe5ca1a4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader_profile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mbatch_idy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mGen_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDis_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset_front\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mD_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGen_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mG_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDis_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-71-686e6487acb3>\u001b[0m in \u001b[0;36mloss\u001b[1;34m(profile, front)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfront\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#=======================Train the generator=======================#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfeature_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# G_enc(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mfeature_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfront\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# G_enc(y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Git\\FNM-with-Extreme-Pose-and-Expression-in-the-Wild\\Resnet50_ft_dag.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[0mconv5_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv5_2x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv5_3_1x1_increase_bn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mconv5_3x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv5_3_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv5_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[0mpool5_7x7_s1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool5_7x7_s1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv5_3x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m         \u001b[0mclassifier_preflatten\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool5_7x7_s1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier_preflatten\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier_preflatten\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\pooling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m         return F.avg_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m--> 554\u001b[1;33m                             self.padding, self.ceil_mode, self.count_include_pad, self.divisor_override)\n\u001b[0m\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given input size: (2048x3x3). Calculated output size: (2048x-3x-3). Output size is too small"
     ]
    }
   ],
   "source": [
    "n_epoch = 200\n",
    "for epoch in range(1, n_epoch+1):           \n",
    "    D_losses, G_losses = [], []\n",
    "    d = list(enumerate(train_loader_front))\n",
    "    i = 0\n",
    "    for batch_idx, (x, _) in enumerate(train_loader_profile):\n",
    "        batch_idy, (y, _) = d[i]\n",
    "        Gen_loss, Dis_loss = loss(x, train_dataset_front[i])\n",
    "        D_losses.append(Gen_loss)\n",
    "        G_losses.append(Dis_loss)\n",
    "        i += 1\n",
    "\n",
    "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
    "            (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
