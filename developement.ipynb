{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_bias = False\n",
    "norm = Batch_norm\n",
    "front_path = './images/fig1'\n",
    "profile_path = '.images/fig2'\n",
    "lambda_l1 = 0.001, #'weight of the loss for L1 texture loss') # 0.001\n",
    "lambda_fea=100 #'weight of the loss for face model feature loss')\n",
    "lambda_reg= 1e-5# 'weight of the loss for L2 regularitaion loss')\n",
    "lambda_gan= 1# 'weight of the loss for gan loss')\n",
    "lambda_gp=10# 'weight of the loss for gradient penalty on parameter of D')\n",
    "\n",
    "# For training\n",
    "dataset_size=  1000# 'dataset path')  # casia_aligned_250_250_jpg\n",
    "profile_list=''# 'train profile list')\n",
    "front_path=''#front data path')\n",
    "front_list=''# 'train front list')\n",
    "test_path=''# 'front data path')\n",
    "is_train=True# 'train or test')\n",
    "is_finetune= False# 'finetune') # False, True\n",
    "face_mode='resnet50.npy'# 'face model path')\n",
    "checkpoint='checkpoint/fnm'# 'checkpoint directory')\n",
    "summary_dir= 'log/fnm'# 'logs directory')\n",
    "checkpoint_ft='checkpoint/fnm/ck-09'#'finetune or test checkpoint path')\n",
    "batch_size= 16# 'batch size')\n",
    "epoc=10 # 'epoch')\n",
    "critic= 1 #'number of D training times')\n",
    "save_freq= 1000 # 'the frequency of saving model')\n",
    "lr=1e-4# 'base learning rate')\n",
    "beta1=0. # 'beta1 momentum term of adam')\n",
    "beta2=0.9 # 'beta2 momentum term of adam')\n",
    "stddev= 0.02 # 'stddev for W initializer')\n",
    "use_bias=False # 'whether to use bias')\n",
    "results='results/fnm' # 'path for saving results') #\n",
    "\n",
    "############################\n",
    "#   environment setting    #\n",
    "############################\n",
    "device_id='3,4'# 'device id')\n",
    "ori_height=224 # 'original height of profile images')\n",
    "ori_width=224 # 'original width of profile images')\n",
    "height= 224 #'height of images') # do not modified\n",
    "width= 224 # 'width of images') # do not modified\n",
    "CHANNEL=3 # 'channel of images')\n",
    "num_threads=8 # 'number of threads of enqueueing examples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50 model trained on VGGFace2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Resnet50_ft_dag(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Resnet50_ft_dag, self).__init__()\n",
    "        self.meta = {'mean': [131.0912, 103.8827, 91.4953],\n",
    "                     'std': [1, 1, 1],\n",
    "                     'imageSize': [224, 224, 3]}\n",
    "        self.conv1_7x7_s2 = nn.Conv2d(3, 64, kernel_size=[7, 7], stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.conv1_7x7_s2_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv1_relu_7x7_s2 = nn.ReLU()\n",
    "        self.pool1_3x3_s2 = nn.MaxPool2d(kernel_size=[3, 3], stride=[2, 2], padding=(0, 0), dilation=1, ceil_mode=True)\n",
    "        self.conv2_1_1x1_reduce = nn.Conv2d(64, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_1_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_1_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv2_1_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv2_1_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_1_3x3_relu = nn.ReLU()\n",
    "        self.conv2_1_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_1_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_1_1x1_proj = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_1_1x1_proj_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_1_relu = nn.ReLU()\n",
    "        self.conv2_2_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_2_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_2_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv2_2_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv2_2_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_2_3x3_relu = nn.ReLU()\n",
    "        self.conv2_2_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_2_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_2_relu = nn.ReLU()\n",
    "        self.conv2_3_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_3_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_3_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv2_3_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv2_3_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_3_3x3_relu = nn.ReLU()\n",
    "        self.conv2_3_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_3_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_3_relu = nn.ReLU()\n",
    "        self.conv3_1_1x1_reduce = nn.Conv2d(256, 128, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv3_1_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_1_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv3_1_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv3_1_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_1_3x3_relu = nn.ReLU()\n",
    "        self.conv3_1_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_1_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_1_1x1_proj = nn.Conv2d(256, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv3_1_1x1_proj_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_1_relu = nn.ReLU()\n",
    "        self.conv3_2_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_2_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_2_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv3_2_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv3_2_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_2_3x3_relu = nn.ReLU()\n",
    "        self.conv3_2_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_2_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_2_relu = nn.ReLU()\n",
    "        self.conv3_3_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_3_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_3_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv3_3_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv3_3_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_3_3x3_relu = nn.ReLU()\n",
    "        self.conv3_3_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_3_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_3_relu = nn.ReLU()\n",
    "        self.conv3_4_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_4_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_4_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv3_4_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv3_4_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_4_3x3_relu = nn.ReLU()\n",
    "        self.conv3_4_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_4_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_4_relu = nn.ReLU()\n",
    "        self.conv4_1_1x1_reduce = nn.Conv2d(512, 256, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv4_1_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_1_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_1_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_1_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_1_3x3_relu = nn.ReLU()\n",
    "        self.conv4_1_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_1_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_1_1x1_proj = nn.Conv2d(512, 1024, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv4_1_1x1_proj_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_1_relu = nn.ReLU()\n",
    "        self.conv4_2_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_2_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_2_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_2_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_2_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_2_3x3_relu = nn.ReLU()\n",
    "        self.conv4_2_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_2_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_2_relu = nn.ReLU()\n",
    "        self.conv4_3_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_3_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_3_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_3_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_3_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_3_3x3_relu = nn.ReLU()\n",
    "        self.conv4_3_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_3_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_3_relu = nn.ReLU()\n",
    "        self.conv4_4_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_4_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_4_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_4_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_4_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_4_3x3_relu = nn.ReLU()\n",
    "        self.conv4_4_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_4_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_4_relu = nn.ReLU()\n",
    "        self.conv4_5_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_5_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_5_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_5_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_5_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_5_3x3_relu = nn.ReLU()\n",
    "        self.conv4_5_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_5_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_5_relu = nn.ReLU()\n",
    "        self.conv4_6_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_6_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_6_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_6_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_6_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_6_3x3_relu = nn.ReLU()\n",
    "        self.conv4_6_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_6_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_6_relu = nn.ReLU()\n",
    "        self.conv5_1_1x1_reduce = nn.Conv2d(1024, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv5_1_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_1_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv5_1_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv5_1_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_1_3x3_relu = nn.ReLU()\n",
    "        self.conv5_1_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_1_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_1_1x1_proj = nn.Conv2d(1024, 2048, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv5_1_1x1_proj_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_1_relu = nn.ReLU()\n",
    "        self.conv5_2_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_2_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_2_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv5_2_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv5_2_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_2_3x3_relu = nn.ReLU()\n",
    "        self.conv5_2_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_2_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_2_relu = nn.ReLU()\n",
    "        self.conv5_3_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_3_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_3_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv5_3_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv5_3_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_3_3x3_relu = nn.ReLU()\n",
    "        self.conv5_3_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_3_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_3_relu = nn.ReLU()\n",
    "        self.pool5_7x7_s1 = nn.AvgPool2d(kernel_size=[7, 7], stride=[1, 1], padding=0)\n",
    "        self.classifier = nn.Conv2d(2048, 8631, kernel_size=[1, 1], stride=(1, 1))\n",
    "\n",
    "    def __call__(self, data):\n",
    "        conv1_7x7_s2 = self.conv1_7x7_s2(data)\n",
    "        conv1_7x7_s2_bn = self.conv1_7x7_s2_bn(conv1_7x7_s2)\n",
    "        conv1_7x7_s2_bnxx = self.conv1_relu_7x7_s2(conv1_7x7_s2_bn)\n",
    "        pool1_3x3_s2 = self.pool1_3x3_s2(conv1_7x7_s2_bnxx)\n",
    "        conv2_1_1x1_reduce = self.conv2_1_1x1_reduce(pool1_3x3_s2)\n",
    "        conv2_1_1x1_reduce_bn = self.conv2_1_1x1_reduce_bn(conv2_1_1x1_reduce)\n",
    "        conv2_1_1x1_reduce_bnxx = self.conv2_1_1x1_reduce_relu(conv2_1_1x1_reduce_bn)\n",
    "        conv2_1_3x3 = self.conv2_1_3x3(conv2_1_1x1_reduce_bnxx)\n",
    "        conv2_1_3x3_bn = self.conv2_1_3x3_bn(conv2_1_3x3)\n",
    "        conv2_1_3x3_bnxx = self.conv2_1_3x3_relu(conv2_1_3x3_bn)\n",
    "        conv2_1_1x1_increase = self.conv2_1_1x1_increase(conv2_1_3x3_bnxx)\n",
    "        conv2_1_1x1_increase_bn = self.conv2_1_1x1_increase_bn(conv2_1_1x1_increase)\n",
    "        conv2_1_1x1_proj = self.conv2_1_1x1_proj(pool1_3x3_s2)\n",
    "        conv2_1_1x1_proj_bn = self.conv2_1_1x1_proj_bn(conv2_1_1x1_proj)\n",
    "        conv2_1 = torch.add(conv2_1_1x1_proj_bn, 1, conv2_1_1x1_increase_bn)\n",
    "        conv2_1x = self.conv2_1_relu(conv2_1)\n",
    "        conv2_2_1x1_reduce = self.conv2_2_1x1_reduce(conv2_1x)\n",
    "        conv2_2_1x1_reduce_bn = self.conv2_2_1x1_reduce_bn(conv2_2_1x1_reduce)\n",
    "        conv2_2_1x1_reduce_bnxx = self.conv2_2_1x1_reduce_relu(conv2_2_1x1_reduce_bn)\n",
    "        conv2_2_3x3 = self.conv2_2_3x3(conv2_2_1x1_reduce_bnxx)\n",
    "        conv2_2_3x3_bn = self.conv2_2_3x3_bn(conv2_2_3x3)\n",
    "        conv2_2_3x3_bnxx = self.conv2_2_3x3_relu(conv2_2_3x3_bn)\n",
    "        conv2_2_1x1_increase = self.conv2_2_1x1_increase(conv2_2_3x3_bnxx)\n",
    "        conv2_2_1x1_increase_bn = self.conv2_2_1x1_increase_bn(conv2_2_1x1_increase)\n",
    "        conv2_2 = torch.add(conv2_1x, 1, conv2_2_1x1_increase_bn)\n",
    "        conv2_2x = self.conv2_2_relu(conv2_2)\n",
    "        conv2_3_1x1_reduce = self.conv2_3_1x1_reduce(conv2_2x)\n",
    "        conv2_3_1x1_reduce_bn = self.conv2_3_1x1_reduce_bn(conv2_3_1x1_reduce)\n",
    "        conv2_3_1x1_reduce_bnxx = self.conv2_3_1x1_reduce_relu(conv2_3_1x1_reduce_bn)\n",
    "        conv2_3_3x3 = self.conv2_3_3x3(conv2_3_1x1_reduce_bnxx)\n",
    "        conv2_3_3x3_bn = self.conv2_3_3x3_bn(conv2_3_3x3)\n",
    "        conv2_3_3x3_bnxx = self.conv2_3_3x3_relu(conv2_3_3x3_bn)\n",
    "        conv2_3_1x1_increase = self.conv2_3_1x1_increase(conv2_3_3x3_bnxx)\n",
    "        conv2_3_1x1_increase_bn = self.conv2_3_1x1_increase_bn(conv2_3_1x1_increase)\n",
    "        conv2_3 = torch.add(conv2_2x, 1, conv2_3_1x1_increase_bn)\n",
    "        conv2_3x = self.conv2_3_relu(conv2_3)\n",
    "        conv3_1_1x1_reduce = self.conv3_1_1x1_reduce(conv2_3x)\n",
    "        conv3_1_1x1_reduce_bn = self.conv3_1_1x1_reduce_bn(conv3_1_1x1_reduce)\n",
    "        conv3_1_1x1_reduce_bnxx = self.conv3_1_1x1_reduce_relu(conv3_1_1x1_reduce_bn)\n",
    "        conv3_1_3x3 = self.conv3_1_3x3(conv3_1_1x1_reduce_bnxx)\n",
    "        conv3_1_3x3_bn = self.conv3_1_3x3_bn(conv3_1_3x3)\n",
    "        conv3_1_3x3_bnxx = self.conv3_1_3x3_relu(conv3_1_3x3_bn)\n",
    "        conv3_1_1x1_increase = self.conv3_1_1x1_increase(conv3_1_3x3_bnxx)\n",
    "        conv3_1_1x1_increase_bn = self.conv3_1_1x1_increase_bn(conv3_1_1x1_increase)\n",
    "        conv3_1_1x1_proj = self.conv3_1_1x1_proj(conv2_3x)\n",
    "        conv3_1_1x1_proj_bn = self.conv3_1_1x1_proj_bn(conv3_1_1x1_proj)\n",
    "        conv3_1 = torch.add(conv3_1_1x1_proj_bn, 1, conv3_1_1x1_increase_bn)\n",
    "        conv3_1x = self.conv3_1_relu(conv3_1)\n",
    "        conv3_2_1x1_reduce = self.conv3_2_1x1_reduce(conv3_1x)\n",
    "        conv3_2_1x1_reduce_bn = self.conv3_2_1x1_reduce_bn(conv3_2_1x1_reduce)\n",
    "        conv3_2_1x1_reduce_bnxx = self.conv3_2_1x1_reduce_relu(conv3_2_1x1_reduce_bn)\n",
    "        conv3_2_3x3 = self.conv3_2_3x3(conv3_2_1x1_reduce_bnxx)\n",
    "        conv3_2_3x3_bn = self.conv3_2_3x3_bn(conv3_2_3x3)\n",
    "        conv3_2_3x3_bnxx = self.conv3_2_3x3_relu(conv3_2_3x3_bn)\n",
    "        conv3_2_1x1_increase = self.conv3_2_1x1_increase(conv3_2_3x3_bnxx)\n",
    "        conv3_2_1x1_increase_bn = self.conv3_2_1x1_increase_bn(conv3_2_1x1_increase)\n",
    "        conv3_2 = torch.add(conv3_1x, 1, conv3_2_1x1_increase_bn)\n",
    "        conv3_2x = self.conv3_2_relu(conv3_2)\n",
    "        conv3_3_1x1_reduce = self.conv3_3_1x1_reduce(conv3_2x)\n",
    "        conv3_3_1x1_reduce_bn = self.conv3_3_1x1_reduce_bn(conv3_3_1x1_reduce)\n",
    "        conv3_3_1x1_reduce_bnxx = self.conv3_3_1x1_reduce_relu(conv3_3_1x1_reduce_bn)\n",
    "        conv3_3_3x3 = self.conv3_3_3x3(conv3_3_1x1_reduce_bnxx)\n",
    "        conv3_3_3x3_bn = self.conv3_3_3x3_bn(conv3_3_3x3)\n",
    "        conv3_3_3x3_bnxx = self.conv3_3_3x3_relu(conv3_3_3x3_bn)\n",
    "        conv3_3_1x1_increase = self.conv3_3_1x1_increase(conv3_3_3x3_bnxx)\n",
    "        conv3_3_1x1_increase_bn = self.conv3_3_1x1_increase_bn(conv3_3_1x1_increase)\n",
    "        conv3_3 = torch.add(conv3_2x, 1, conv3_3_1x1_increase_bn)\n",
    "        conv3_3x = self.conv3_3_relu(conv3_3)\n",
    "        conv3_4_1x1_reduce = self.conv3_4_1x1_reduce(conv3_3x)\n",
    "        conv3_4_1x1_reduce_bn = self.conv3_4_1x1_reduce_bn(conv3_4_1x1_reduce)\n",
    "        conv3_4_1x1_reduce_bnxx = self.conv3_4_1x1_reduce_relu(conv3_4_1x1_reduce_bn)\n",
    "        conv3_4_3x3 = self.conv3_4_3x3(conv3_4_1x1_reduce_bnxx)\n",
    "        conv3_4_3x3_bn = self.conv3_4_3x3_bn(conv3_4_3x3)\n",
    "        conv3_4_3x3_bnxx = self.conv3_4_3x3_relu(conv3_4_3x3_bn)\n",
    "        conv3_4_1x1_increase = self.conv3_4_1x1_increase(conv3_4_3x3_bnxx)\n",
    "        conv3_4_1x1_increase_bn = self.conv3_4_1x1_increase_bn(conv3_4_1x1_increase)\n",
    "        conv3_4 = torch.add(conv3_3x, 1, conv3_4_1x1_increase_bn)\n",
    "        conv3_4x = self.conv3_4_relu(conv3_4)\n",
    "        conv4_1_1x1_reduce = self.conv4_1_1x1_reduce(conv3_4x)\n",
    "        conv4_1_1x1_reduce_bn = self.conv4_1_1x1_reduce_bn(conv4_1_1x1_reduce)\n",
    "        conv4_1_1x1_reduce_bnxx = self.conv4_1_1x1_reduce_relu(conv4_1_1x1_reduce_bn)\n",
    "        conv4_1_3x3 = self.conv4_1_3x3(conv4_1_1x1_reduce_bnxx)\n",
    "        conv4_1_3x3_bn = self.conv4_1_3x3_bn(conv4_1_3x3)\n",
    "        conv4_1_3x3_bnxx = self.conv4_1_3x3_relu(conv4_1_3x3_bn)\n",
    "        conv4_1_1x1_increase = self.conv4_1_1x1_increase(conv4_1_3x3_bnxx)\n",
    "        conv4_1_1x1_increase_bn = self.conv4_1_1x1_increase_bn(conv4_1_1x1_increase)\n",
    "        conv4_1_1x1_proj = self.conv4_1_1x1_proj(conv3_4x)\n",
    "        conv4_1_1x1_proj_bn = self.conv4_1_1x1_proj_bn(conv4_1_1x1_proj)\n",
    "        conv4_1 = torch.add(conv4_1_1x1_proj_bn, 1, conv4_1_1x1_increase_bn)\n",
    "        conv4_1x = self.conv4_1_relu(conv4_1)\n",
    "        conv4_2_1x1_reduce = self.conv4_2_1x1_reduce(conv4_1x)\n",
    "        conv4_2_1x1_reduce_bn = self.conv4_2_1x1_reduce_bn(conv4_2_1x1_reduce)\n",
    "        conv4_2_1x1_reduce_bnxx = self.conv4_2_1x1_reduce_relu(conv4_2_1x1_reduce_bn)\n",
    "        conv4_2_3x3 = self.conv4_2_3x3(conv4_2_1x1_reduce_bnxx)\n",
    "        conv4_2_3x3_bn = self.conv4_2_3x3_bn(conv4_2_3x3)\n",
    "        conv4_2_3x3_bnxx = self.conv4_2_3x3_relu(conv4_2_3x3_bn)\n",
    "        conv4_2_1x1_increase = self.conv4_2_1x1_increase(conv4_2_3x3_bnxx)\n",
    "        conv4_2_1x1_increase_bn = self.conv4_2_1x1_increase_bn(conv4_2_1x1_increase)\n",
    "        conv4_2 = torch.add(conv4_1x, 1, conv4_2_1x1_increase_bn)\n",
    "        conv4_2x = self.conv4_2_relu(conv4_2)\n",
    "        conv4_3_1x1_reduce = self.conv4_3_1x1_reduce(conv4_2x)\n",
    "        conv4_3_1x1_reduce_bn = self.conv4_3_1x1_reduce_bn(conv4_3_1x1_reduce)\n",
    "        conv4_3_1x1_reduce_bnxx = self.conv4_3_1x1_reduce_relu(conv4_3_1x1_reduce_bn)\n",
    "        conv4_3_3x3 = self.conv4_3_3x3(conv4_3_1x1_reduce_bnxx)\n",
    "        conv4_3_3x3_bn = self.conv4_3_3x3_bn(conv4_3_3x3)\n",
    "        conv4_3_3x3_bnxx = self.conv4_3_3x3_relu(conv4_3_3x3_bn)\n",
    "        conv4_3_1x1_increase = self.conv4_3_1x1_increase(conv4_3_3x3_bnxx)\n",
    "        conv4_3_1x1_increase_bn = self.conv4_3_1x1_increase_bn(conv4_3_1x1_increase)\n",
    "        conv4_3 = torch.add(conv4_2x, 1, conv4_3_1x1_increase_bn)\n",
    "        conv4_3x = self.conv4_3_relu(conv4_3)\n",
    "        conv4_4_1x1_reduce = self.conv4_4_1x1_reduce(conv4_3x)\n",
    "        conv4_4_1x1_reduce_bn = self.conv4_4_1x1_reduce_bn(conv4_4_1x1_reduce)\n",
    "        conv4_4_1x1_reduce_bnxx = self.conv4_4_1x1_reduce_relu(conv4_4_1x1_reduce_bn)\n",
    "        conv4_4_3x3 = self.conv4_4_3x3(conv4_4_1x1_reduce_bnxx)\n",
    "        conv4_4_3x3_bn = self.conv4_4_3x3_bn(conv4_4_3x3)\n",
    "        conv4_4_3x3_bnxx = self.conv4_4_3x3_relu(conv4_4_3x3_bn)\n",
    "        conv4_4_1x1_increase = self.conv4_4_1x1_increase(conv4_4_3x3_bnxx)\n",
    "        conv4_4_1x1_increase_bn = self.conv4_4_1x1_increase_bn(conv4_4_1x1_increase)\n",
    "        conv4_4 = torch.add(conv4_3x, 1, conv4_4_1x1_increase_bn)\n",
    "        conv4_4x = self.conv4_4_relu(conv4_4)\n",
    "        conv4_5_1x1_reduce = self.conv4_5_1x1_reduce(conv4_4x)\n",
    "        conv4_5_1x1_reduce_bn = self.conv4_5_1x1_reduce_bn(conv4_5_1x1_reduce)\n",
    "        conv4_5_1x1_reduce_bnxx = self.conv4_5_1x1_reduce_relu(conv4_5_1x1_reduce_bn)\n",
    "        conv4_5_3x3 = self.conv4_5_3x3(conv4_5_1x1_reduce_bnxx)\n",
    "        conv4_5_3x3_bn = self.conv4_5_3x3_bn(conv4_5_3x3)\n",
    "        conv4_5_3x3_bnxx = self.conv4_5_3x3_relu(conv4_5_3x3_bn)\n",
    "        conv4_5_1x1_increase = self.conv4_5_1x1_increase(conv4_5_3x3_bnxx)\n",
    "        conv4_5_1x1_increase_bn = self.conv4_5_1x1_increase_bn(conv4_5_1x1_increase)\n",
    "        conv4_5 = torch.add(conv4_4x, 1, conv4_5_1x1_increase_bn)\n",
    "        conv4_5x = self.conv4_5_relu(conv4_5)\n",
    "        conv4_6_1x1_reduce = self.conv4_6_1x1_reduce(conv4_5x)\n",
    "        conv4_6_1x1_reduce_bn = self.conv4_6_1x1_reduce_bn(conv4_6_1x1_reduce)\n",
    "        conv4_6_1x1_reduce_bnxx = self.conv4_6_1x1_reduce_relu(conv4_6_1x1_reduce_bn)\n",
    "        conv4_6_3x3 = self.conv4_6_3x3(conv4_6_1x1_reduce_bnxx)\n",
    "        conv4_6_3x3_bn = self.conv4_6_3x3_bn(conv4_6_3x3)\n",
    "        conv4_6_3x3_bnxx = self.conv4_6_3x3_relu(conv4_6_3x3_bn)\n",
    "        conv4_6_1x1_increase = self.conv4_6_1x1_increase(conv4_6_3x3_bnxx)\n",
    "        conv4_6_1x1_increase_bn = self.conv4_6_1x1_increase_bn(conv4_6_1x1_increase)\n",
    "        conv4_6 = torch.add(conv4_5x, 1, conv4_6_1x1_increase_bn)\n",
    "        conv4_6x = self.conv4_6_relu(conv4_6)\n",
    "        conv5_1_1x1_reduce = self.conv5_1_1x1_reduce(conv4_6x)\n",
    "        conv5_1_1x1_reduce_bn = self.conv5_1_1x1_reduce_bn(conv5_1_1x1_reduce)\n",
    "        conv5_1_1x1_reduce_bnxx = self.conv5_1_1x1_reduce_relu(conv5_1_1x1_reduce_bn)\n",
    "        conv5_1_3x3 = self.conv5_1_3x3(conv5_1_1x1_reduce_bnxx)\n",
    "        conv5_1_3x3_bn = self.conv5_1_3x3_bn(conv5_1_3x3)\n",
    "        conv5_1_3x3_bnxx = self.conv5_1_3x3_relu(conv5_1_3x3_bn)\n",
    "        conv5_1_1x1_increase = self.conv5_1_1x1_increase(conv5_1_3x3_bnxx)\n",
    "        conv5_1_1x1_increase_bn = self.conv5_1_1x1_increase_bn(conv5_1_1x1_increase)\n",
    "        conv5_1_1x1_proj = self.conv5_1_1x1_proj(conv4_6x)\n",
    "        conv5_1_1x1_proj_bn = self.conv5_1_1x1_proj_bn(conv5_1_1x1_proj)\n",
    "        conv5_1 = torch.add(conv5_1_1x1_proj_bn, 1, conv5_1_1x1_increase_bn)\n",
    "        conv5_1x = self.conv5_1_relu(conv5_1)\n",
    "        conv5_2_1x1_reduce = self.conv5_2_1x1_reduce(conv5_1x)\n",
    "        conv5_2_1x1_reduce_bn = self.conv5_2_1x1_reduce_bn(conv5_2_1x1_reduce)\n",
    "        conv5_2_1x1_reduce_bnxx = self.conv5_2_1x1_reduce_relu(conv5_2_1x1_reduce_bn)\n",
    "        conv5_2_3x3 = self.conv5_2_3x3(conv5_2_1x1_reduce_bnxx)\n",
    "        conv5_2_3x3_bn = self.conv5_2_3x3_bn(conv5_2_3x3)\n",
    "        conv5_2_3x3_bnxx = self.conv5_2_3x3_relu(conv5_2_3x3_bn)\n",
    "        conv5_2_1x1_increase = self.conv5_2_1x1_increase(conv5_2_3x3_bnxx)\n",
    "        conv5_2_1x1_increase_bn = self.conv5_2_1x1_increase_bn(conv5_2_1x1_increase)\n",
    "        conv5_2 = torch.add(conv5_1x, 1, conv5_2_1x1_increase_bn)\n",
    "        conv5_2x = self.conv5_2_relu(conv5_2)\n",
    "        conv5_3_1x1_reduce = self.conv5_3_1x1_reduce(conv5_2x)\n",
    "        conv5_3_1x1_reduce_bn = self.conv5_3_1x1_reduce_bn(conv5_3_1x1_reduce)\n",
    "        conv5_3_1x1_reduce_bnxx = self.conv5_3_1x1_reduce_relu(conv5_3_1x1_reduce_bn)\n",
    "        conv5_3_3x3 = self.conv5_3_3x3(conv5_3_1x1_reduce_bnxx)\n",
    "        conv5_3_3x3_bn = self.conv5_3_3x3_bn(conv5_3_3x3)\n",
    "        conv5_3_3x3_bnxx = self.conv5_3_3x3_relu(conv5_3_3x3_bn)\n",
    "        conv5_3_1x1_increase = self.conv5_3_1x1_increase(conv5_3_3x3_bnxx)\n",
    "        conv5_3_1x1_increase_bn = self.conv5_3_1x1_increase_bn(conv5_3_1x1_increase)\n",
    "        conv5_3 = torch.add(conv5_2x, 1, conv5_3_1x1_increase_bn)\n",
    "        conv5_3x = self.conv5_3_relu(conv5_3)\n",
    "        pool5_7x7_s1 = self.pool5_7x7_s1(conv5_3x)\n",
    "        classifier_preflatten = self.classifier(pool5_7x7_s1)\n",
    "        classifier = classifier_preflatten.view(classifier_preflatten.size(0), -1)\n",
    "        return conv3_4, conv4_6, conv5_3, pool5_7x7_s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_norm(nn.Module):\n",
    "    \n",
    "    def __init__(self, epsilon=1e-5, momentum = 0.9):\n",
    "        # self.mean  = mosv_dict['mean']\n",
    "        self.epsilon  = epsilon\n",
    "        self.momentum = momentum\n",
    "        #self.scale = mosv_dict['scale']\n",
    "        #self.variance = mosv_dict['variance']\n",
    "        #self.epsilon = 1e-5\n",
    "    def __call__(self, x, is_train):\n",
    "        return nn.BatchNorm1d(x,eps = self.epsilon,  momentum = self.momentum, track_running_stats = is_train)\n",
    "            \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options: conv2d, res_block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_conv_pad(input_size, output_size, filter_size, stride):\n",
    "    return math.floor((stride * (output_size - 1) + filter_size - input_size)/2 + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_deconv_pad(input_size, output_size, filter_size, stride):\n",
    "    return math.floor((stride * (input_size - 1) + filter_size - output_size)/2 + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def conv2d(inputs ,filters,  kernel_size, strides = 1, padding=0, bias=use_bias,\n",
    "           dilation_rate = 1, activation = None, init = False):\n",
    "     #\"\"\"Convolution layer\"\"\"\n",
    "        X = F.conv2d(inputs, weight, bias=bias, kernel_size = kernel_size, stride=strides, padding=padding, dilation=dilation_rate)\n",
    "        if (activation is not None):\n",
    "            X =  activation(X);\n",
    "        return X;\n",
    "\n",
    "\n",
    "def deconv2d(inputs, filters, name, kernel_size = 3, strides = 1, padding=0,\n",
    "             trainable = True, activation = None, bias=use_bias):\n",
    "    #\"\"\"Deconv layer\"\"\"\n",
    "        X = nn.ConvTranspose2d(in_channels, out_channels, kernel_size =  kernel_size, stride=strides, \n",
    "                      padding=padding, bias=bias)\n",
    "        if (activation is not None):\n",
    "            X = activation(X);\n",
    "        return X;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d( in_channels , out_channels , kernel_size = 3 , stride = 1  , \n",
    "           padding  =  None, \n",
    "           bias = use_bias,dilation_rate = 1, activation = None):\n",
    "    layers = []\n",
    "    if padding == None:\n",
    "        padding = calc_conv_pad(in_channels , out_channels, kernel_size , stride)\n",
    "    conv = nn.Conv2d( in_channels , out_channels , bias=bias, kernel_size = kernel_size, stride=strides, padding=padding, dilation=dilation_rate )\n",
    "    layers.append( conv )\n",
    "    if activation is not None:\n",
    "        layers.append( activation )\n",
    "    return nn.Sequential( *layers )\n",
    "\n",
    "def deconv2d(in_channels , out_channels , kernel_size = 3 , stride = 1  , \n",
    "             padding  = None,\n",
    "             bias= use_bias, dilation_rate = 1,\n",
    "            activation = None):\n",
    "    if padding == None:\n",
    "        padding = calc_deconv_pad(in_channels , out_channels, kernel_size , stride)\n",
    "    layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size =  kernel_size, stride=stride, \n",
    "                      padding=padding, bias=bias)]\n",
    "    if activation is not None:\n",
    "        layers.append( activation )\n",
    "    return nn.Sequential( *layers )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,  in_channels , \n",
    "                 out_channels , kernel_size = 3, \n",
    "                 stride = 1  , padding  = 0 ,\n",
    "                 bias = use_bias,  norm = Batch_norm, \n",
    "                 activation = None, activation2 = nn.ReLU()):\n",
    "        self.out_channels = out_channels\n",
    "        self.use_projection = use_projection\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.activation = activation2\n",
    "        convs = [conv2d(in_channels , out_channels , kernel_size , stride , padding, bias, activation = activation), \n",
    "             norm, \n",
    "             nn.ReLU(), \n",
    "             conv2d(out_channels, out_channels, kernel_size, stride, padding, bias, activation = activation),\n",
    "             norm]\n",
    "        self.layers = nn.Sequential(*convs)\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.layers(x) + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f7_shape = [7, 7, 2048]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### second variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self): #, profile, front, train):\n",
    "        super(Generator, self).__init__() \n",
    "#         self.face_model = Resnet50_ft_dag()\n",
    "#         self.feature_p = self.face_model.forward(profile)\n",
    "#         self.feature_f = self.face_model.forward(front)\n",
    "#         self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
    "#         self.is_train = train\n",
    "        \n",
    "        self.conv1 = conv2d(2048, 512, kernel_size=1, strides = 1)\n",
    "        self.norm1 = norm(512, self.is_train)\n",
    "        self.res1_1 = res_block(512, 512, norm = NORM)\n",
    "        self.res1_2 = res_block(512, 512, norm = NORM)\n",
    "        self.res1_3 = res_block(512, 512, norm = NORM)\n",
    "        self.res1_4 = res_block(512, 512, norm = NORM)\n",
    "        \n",
    "        self.dconv2 = nn.Sequential(deconv2d(512, 256, kernel_size=4, strides = 2), NORM(), nn.ReLU())\n",
    "        self.res2 = res_block(256, 256, norm = NORM)\n",
    "        self.dconv3 = nn.Sequential(deconv2d(256, 128, kernel_size=4, strides = 2), NORM(), nn.ReLU())\n",
    "        self.res3 = res_block(128, 128, norm = NORM)\n",
    "        self.dconv4 = nn.Sequential(deconv2d(128, 64, kernel_size=4, strides = 2), NORM(), nn.ReLU())\n",
    "        self.res4 = res_block(64, 64, norm = NORM)\n",
    "        self.dconv5 = nn.Sequential(deconv2d(64, 32, kernel_size=4, strides = 2), NORM(), nn.ReLU())\n",
    "        self.res5 = res_block(32, 32, norm = NORM)\n",
    "        self.dconv6 = nn.Sequential(deconv2d(32, 32, kernel_size=4, strides = 2), NORM(), nn.ReLU())\n",
    "        self.res6 = res_block(32, 32, norm = NORM)\n",
    "        self.gen = nn.Sequential(conv2d(32, 3,  kernel_size=1, strides = 1), nn.Tanh)\n",
    "        \n",
    "    def forward(self, feature):\n",
    "        feat28, feat14, feat7, pool5 = feature[0],feature[1],feature[2],feature[3]\n",
    "        conv1 = self.conv1(feat7)\n",
    "        norm1 = self.norm1(conv1)\n",
    "        relu1 = self.relu1(norm1)\n",
    "        res1_1 = self.res1_1(relu1)\n",
    "        res1_2 = self.res1_2(res1_1)\n",
    "        res1_3 = self.res1_3(res1_2)\n",
    "        res1_4 = self.res1_2(res1_3)\n",
    "        dconv2 = self.deconv2(res1_4)\n",
    "        res2 = self.res2(dconv2)\n",
    "        dconv3 = self.dconv3(res2)\n",
    "        res3 = self.res3(dconv3)\n",
    "        dconv4 = self.dconv4(res3)\n",
    "        res4 = self.res4(dconv4)\n",
    "        dconv5 = self.dconv5(res4)\n",
    "        res5 = self.res5(dconv5)\n",
    "        dconv6 = self.dconv6(res5)\n",
    "        res6 = self.res6(dconv6)\n",
    "        gen = self.gen(res6)\n",
    "        return (gen + 1)* 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS =  bs = images.get_shape().as_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.layers.dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.h0_0 = nn.Sequential(conv2d(3, 32, kernel_size=3, strides=2), nn.LeakyReLU())\n",
    "        self.h0_1 = nn.Sequential(conv2d(32, 64, kernel_size=3, strides=2), NORM(), nn.LeakyReLu())\n",
    "        self.h0_2 = nn.Sequential(conv2d(64, 128, kernel_size=3, strides=2), NORM(), nn.LeakyReLu())\n",
    "        self.h0_3 = nn.Sequential(conv2d(128, 256, kernel_size=3, strides=2), NORM(), nn.LeakyReLu())\n",
    "        self.h0_4 = nn.Sequential(conv2d(256, 256, kernel_size=3, strides=2), NORM(), nn.LeakyReLu(), torch.reshape([BS, -1]))\n",
    "        self.h0_5 = nn.Linear(self.h0_4.out_features, 1)\n",
    "        \n",
    "        self.h1_0 = nn.Sequential(conv2d(3, 32, kernel_size=3, strides=2), nn.LeakyReLU())\n",
    "        self.h1_1 = nn.Sequential(conv2d(32, 64, kernel_size=3, strides=2), NORM(), nn.LeakyReLu())\n",
    "        self.h1_2 = nn.Sequential(conv2d(64, 128, kernel_size=3, strides=2), NORM(), nn.LeakyReLu())\n",
    "        self.h1_3 = nn.Sequential(conv2d(128, 256, kernel_size=3, strides=2), NORM(), nn.LeakyReLu(), torch.reshape([BS, -1]))\n",
    "        self.h1_4 = nn.Linear(self.h1_3.out_features, 1)\n",
    "        \n",
    "        self.h2_0 = nn.Sequential(conv2d(3, 32, kernel_size=3, strides=2), nn.LeakyReLU())\n",
    "        self.h2_1 = nn.Sequential(conv2d(32, 64, kernel_size=3, strides=2), NORM(), nn.LeakyReLu())\n",
    "        self.h2_2 = nn.Sequential(conv2d(64, 128, kernel_size=3, strides=2), NORM(), nn.LeakyReLu())\n",
    "        self.h2_3 = nn.Sequential(conv2d(128, 256, kernel_size=3, strides=2), NORM(), nn.LeakyReLu(), torch.reshape([BS, -1]))\n",
    "        self.h2_4 = nn.Linear(self.h2_3.out_features, 1)\n",
    "        \n",
    "        self.h3_0 = nn.Sequential(conv2d(3, 32, kernel_size=3, strides=2), nn.LeakyReLU())\n",
    "        self.h3_1 = nn.Sequential(conv2d(32, 64, kernel_size=3, strides=2), NORM(), nn.LeakyReLu())\n",
    "        self.h3_2 = nn.Sequential(conv2d(64, 128, kernel_size=3, strides=2), NORM(), nn.LeakyReLu())\n",
    "        self.h3_3 = nn.Sequential(conv2d(128, 256, kernel_size=3, strides=2), NORM(), nn.LeakyReLu(), torch.reshape([BS, -1]))\n",
    "        self.h3_4 = nn.Linear(self.h3_3.out_features, 1)\n",
    "        \n",
    "        self.h4_0 = nn.Sequential(conv2d(3, 32, kernel_size=3, strides=2), nn.LeakyReLU())\n",
    "        self.h4_1 = nn.Sequential(conv2d(32, 64, kernel_size=3, strides=2), NORM(), nn.LeakyReLu())\n",
    "        self.h4_2 = nn.Sequential(conv2d(64, 128, kernel_size=3, strides=2), NORM(), nn.LeakyReLu())\n",
    "        self.h4_3 = nn.Sequential(conv2d(128, 256, kernel_size=3, strides=2), NORM(), nn.LeakyReLu(), torch.reshape([BS, -1]))\n",
    "        self.h4_4 = nn.Linear(self.h4_3.out_features, 1)\n",
    "    \n",
    "    \n",
    "    def forward(self, images):\n",
    "        eyes = tf.slice(images, [0,64,50,0], [bs,36,124,cfg.channel])\n",
    "        nose = tf.slice(images, [0,75,90,0], [bs,65,44,cfg.channel])\n",
    "        mouth = tf.slice(images, [0,140,75,0], [bs,30,74,cfg.channel])\n",
    "        face = tf.slice(images, [0,64,50,0], [bs,116,124,cfg.channel])\n",
    "        h0_0 = self.h0_0(images)\n",
    "        h0_1 = self.h0_1(h0_0)\n",
    "        h0_2 = self.h0_2(h0_1)\n",
    "        h0_3 = self.h0_3(h0_2)\n",
    "        h0_4 = self.h0_4(h0_3)\n",
    "        h0_5 = self.h0_5(h0_4)\n",
    "        \n",
    "        h1_0 = self.h1_0(eyes)\n",
    "        h1_1 = self.h1_1(h1_0)\n",
    "        h1_2 = self.h1_2(h1_1)\n",
    "        h1_3 = self.h1_3(h1_2)\n",
    "        h1_4 = self.h1_4(h1_3)\n",
    "        \n",
    "        h2_0 = self.h2_0(nose)\n",
    "        h2_1 = self.h2_1(h2_0)\n",
    "        h2_2 = self.h2_2(h2_1)\n",
    "        h2_3 = self.h2_3(h2_2)\n",
    "        h2_4 = self.h2_4(h2_3)\n",
    "        \n",
    "        h3_0 = self.h3_0(mouth)\n",
    "        h3_1 = self.h3_1(h3_0)\n",
    "        h3_2 = self.h3_2(h3_1)\n",
    "        h3_3 = self.h3_3(h3_2)\n",
    "        h3_4 = self.h3_4(h3_3)\n",
    "        \n",
    "        h4_0 = self.h4_0(face)\n",
    "        h4_1 = self.h4_1(h4_0)\n",
    "        h4_2 = self.h4_2(h4_1)\n",
    "        h4_3 = self.h4_3(h4_2)\n",
    "        h4_4 = self.h4_4(h4_3)\n",
    "        return h0_5, h1_4, h2_4, h3_4, h4_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = Resnet50_ft_dag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_train(profile, front):\n",
    "    #=======================Train the generator=======================#\n",
    "    feature_p = Model.forward(profile)\n",
    "    feature_f = Model.forward(front)\n",
    "    G.zero_grad()\n",
    "    gen_p = G.forward(feature_p)\n",
    "    gen_f = G.forward(feature_f)\n",
    "    feature_gen_p = Model.forward(gen_p)\n",
    "    feature_gen_f = Model.forward(gen_f)\n",
    "    \n",
    "    pool5_p_norm = feature_p[-1]/()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#     z = Variable(torch.randn(bs, z_dim).to(device))\n",
    "#     y = Variable(torch.ones(bs, 1).to(device))\n",
    "\n",
    "#     G_output = G(z)\n",
    "#     D_output = D(G_output)\n",
    "#     G_loss = criterion(D_output, y)\n",
    "\n",
    "#     # gradient backprop & optimize ONLY G's parameters\n",
    "#     G_loss.backward()\n",
    "#     G_optimizer.step()\n",
    "        \n",
    "#     return G_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def D_train(x):\n",
    "#     #=======================Train the discriminator=======================#\n",
    "#     D.zero_grad()\n",
    "\n",
    "#     # train discriminator on real\n",
    "#     x_real, y_real = x.view(-1, mnist_dim), torch.ones(bs, 1)\n",
    "#     x_real, y_real = Variable(x_real.to(device)), Variable(y_real.to(device))\n",
    "\n",
    "#     D_output = D(x_real)\n",
    "#     D_real_loss = criterion(D_output, y_real)\n",
    "#     D_real_score = D_output\n",
    "\n",
    "#     # train discriminator on facke\n",
    "#     z = Variable(torch.randn(bs, z_dim).to(device))\n",
    "#     x_fake, y_fake = G(z), Variable(torch.zeros(bs, 1).to(device))\n",
    "\n",
    "#     D_output = D(x_fake)\n",
    "#     D_fake_loss = criterion(D_output, y_fake)\n",
    "#     D_fake_score = D_output\n",
    "\n",
    "#     # gradient backprop & optimize ONLY D's parameters\n",
    "#     D_loss = D_real_loss + D_fake_loss\n",
    "#     D_loss.backward()\n",
    "#     D_optimizer.step()\n",
    "        \n",
    "#     return  D_loss.data.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
