{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "from PIL import Image\n",
    "from PIL import __version__\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\d.baranovska\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\d.baranovska\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\d.baranovska\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\d.baranovska\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\d.baranovska\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\d.baranovska\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('board/fnm_exp1')\n",
    "\n",
    "#!tensorboard --logdir I:\\tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from PIL import __version__\n",
    "except ImportError:\n",
    "    from PIL import PILLOW_VERSION as __version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d.baranovska\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\d.baranovska\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\d.baranovska\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\d.baranovska\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\d.baranovska\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\d.baranovska\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_bias = False\n",
    "front_path = './images/fig1'\n",
    "profile_path = '.images/fig2'\n",
    "lambda_l1 = 0.005 #'weight of the loss for L1 texture loss') # 0.001\n",
    "lambda_fea=100 #'weight of the loss for face model feature loss')\n",
    "lambda_reg= 1e-5# 'weight of the loss for L2 regularitaion loss')\n",
    "lambda_gan= 1# 'weight of the loss for gan loss')\n",
    "lambda_gp=10# 'weight of the loss for gradient penalty on parameter of D')\n",
    "\n",
    "# For training\n",
    "dataset_size=  1000# 'dataset path')  # casia_aligned_250_250_jpg\n",
    "profile_list=''# 'train profile list')\n",
    "front_path=''#front data path')\n",
    "front_list=''# 'train front list')\n",
    "test_path=''# 'front data path')\n",
    "is_train=True# 'train or test')\n",
    "is_finetune= False# 'finetune') # False, True\n",
    "face_mode='resnet50.npy'# 'face model path')\n",
    "checkpoint='checkpoint/fnm'# 'checkpoint directory')\n",
    "summary_dir= 'log/fnm'# 'logs directory')\n",
    "checkpoint_ft='checkpoint/fnm/ck-09'#'finetune or test checkpoint path')\n",
    "batch_size= 4# 'batch size')#was 16\n",
    "BS = batch_size\n",
    "epoc=10 # 'epoch')\n",
    "critic= 1 #'number of D training times')\n",
    "save_freq= 1000 # 'the frequency of saving model')\n",
    "lr=1e-4# 'base learning rate')\n",
    "beta1=0. # 'beta1 momentum term of adam')\n",
    "beta2=0.9 # 'beta2 momentum term of adam')\n",
    "stddev= 0.02 # 'stddev for W initializer')\n",
    "use_bias=False # 'whether to use bias')\n",
    "results='results/fnm' # 'path for saving results') #\n",
    "############################\n",
    "############################\n",
    "#   environment setting    #\n",
    "############################\n",
    "device_id='3,4'# 'device id')\n",
    "ori_height=224 # 'original height of profile images')\n",
    "ori_width=224 # 'original width of profile images')\n",
    "height= 224 #'height of images') # do not modified\n",
    "width= 224 # 'width of images') # do not modified\n",
    "CHANNEL=3 # 'channel of images')\n",
    "num_threads=8 # 'number of threads of enqueueing examples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50 model trained on VGGFace2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Resnet50_ft_dag import resnet50_ft_dag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path= \"./resnet50_ft_dag.pth\"\n",
    "Model = resnet50_ft_dag(weights_path=weights_path).to(device)\n",
    "for p in Model.parameters():\n",
    "        p.require_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"FNM\", comment = \"19.03.2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Batch_norm(nn.Module):\n",
    "#     def __init__(self, in_channels, epsilon=1e-5, momentum = 0.9, is_train = False):\n",
    "#         super(Batch_norm, self).__init__()\n",
    "#         # self.mean  = mosv_dict['mean']\n",
    "#         self.norm = nn.BatchNorm2d(in_channels, eps = epsilon,  momentum = momentum, track_running_stats = is_train)\n",
    "#         #self.scale = mosv_dict['scale']\n",
    "#         #self.variance = mosv_dict['variance']\n",
    "#         #self.epsilon = 1e-5\n",
    "#     def forward(self, x):\n",
    "#         norm = self.norm(x)\n",
    "#         return norm\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Batch_norm(in_channels, epsilon=1e-5, momentum = 0.9, is_train = False):\n",
    "    return nn.BatchNorm2d(in_channels, eps = epsilon,  momentum = momentum, track_running_stats = is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM = Batch_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options: conv2d, res_block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_conv_pad(input_size, output_size, filter_size = 3, stride = 2):\n",
    "    return max(0, floor((stride * (output_size - 1) + filter_size - input_size)/2 + 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_deconv_pad(input_size, output_size, filter_size = 3, stride = 2):\n",
    "    return max(0, floor((stride * (input_size - 1) + filter_size - output_size)/2 + 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d( in_channels , out_channels , kernel_size = 3 , strides = 1  , \n",
    "           padding  =  None,  pad_input = None,\n",
    "           bias = use_bias,dilation_rate = 1, activation = None):\n",
    "    layers = []\n",
    "    if padding == None:\n",
    "        if pad_input is not None:\n",
    "            padding = calc_conv_pad(pad_input , 2*pad_input, kernel_size , strides)\n",
    "        else:\n",
    "            padding = 0\n",
    "    conv = nn.Conv2d( in_channels , out_channels , bias=bias, kernel_size = kernel_size, stride=strides, padding=padding, dilation=dilation_rate )\n",
    "    layers.append( conv )\n",
    "    if activation is not None:\n",
    "        layers.append( activation )\n",
    "    return nn.Sequential( *layers )\n",
    "\n",
    "def deconv2d(in_channels , out_channels , kernel_size = 3 , strides = 1  , \n",
    "             padding  = None, pad_input = None,\n",
    "             bias= use_bias, dilation_rate = 1,\n",
    "            activation = None):\n",
    "    if padding == None:\n",
    "        if pad_input is not None:\n",
    "            padding = calc_deconv_pad(pad_input , 2*pad_input, kernel_size , strides)\n",
    "        else:\n",
    "            padding = 0\n",
    "    layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size =  kernel_size, stride=strides, \n",
    "                      padding=padding, bias=bias)]\n",
    "    if activation is not None:\n",
    "        layers.append( activation )\n",
    "    return nn.Sequential( *layers )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class res_block(nn.Module):\n",
    "    def __init__(self,  in_channels , \n",
    "                 out_channels , kernel_size = 3, \n",
    "                 stride = 1  , padding  =  None, pad_input = None,\n",
    "                 bias = use_bias,  norm = NORM, activation2 = nn.ReLU, activation = None,):\n",
    "        super(res_block, self).__init__()\n",
    "        if padding == None:\n",
    "            if pad_input is not None:\n",
    "                padding = calc_conv_pad(pad_input , pad_input, kernel_size , stride)\n",
    "            else: \n",
    "                padding = 0\n",
    "        self.out_channels = out_channels\n",
    "        if activation is not None:\n",
    "            self.activation = activation(out_channels)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "        self.activation2 = activation2(out_channels)\n",
    "        convs = [conv2d(in_channels , out_channels , kernel_size , stride , padding, bias, activation = self.activation), \n",
    "             nn.BatchNorm2d( in_channels ), \n",
    "             nn.ReLU(out_channels), \n",
    "             conv2d(out_channels, out_channels, kernel_size, stride, padding, bias, activation = self.activation),\n",
    "             norm(in_channels)]\n",
    "        self.layers = nn.Sequential(*convs)\n",
    "        #независимо от передаваемого параметра NORM норма всегда Batch_norm\n",
    "    def forward(self, x):\n",
    "        return self.activation2(self.layers(x) + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "src_dir = \"your/source/dir\"\n",
    "dst_dir = \"your/destination/dir\"\n",
    "profile_set = set()\n",
    "profile_list = []\n",
    "front_set = set()\n",
    "front_list = []\n",
    "#frontFile = open(\"./TPdata/train_data/front_list.txt\", \"w\")\n",
    "#profileFile = open(\"./TPdata/train_data/profile_list.txt\", \"w\")\n",
    "\n",
    "def relocate_data(src_dir, src_dir_front, src_dir_profile, variety = 3):\n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.png\")):\n",
    "        tmp = jpgfile.split('\\\\')[-1].split('.')[0].split('_')\n",
    "        ttmp = tmp[0]+ \"_\"+tmp[2]\n",
    "        if (len(tmp) > 6):\n",
    "            if ttmp not in profile_set or random()< 1/2:\n",
    "                shutil.copy(jpgfile, src_dir_profile)\n",
    "                profileFile.write(jpgfile+\"\\n\")\n",
    "                #profile_list.append(jpgfile)\n",
    "                profile_set.add(ttmp)\n",
    "        else:\n",
    "            if ttmp not in front_set or random()< 3/7:\n",
    "                shutil.copy(jpgfile, src_dir_front)\n",
    "                frontFile.write(jpgfile+\"\\n\")\n",
    "                #front_list.append(jpgfile)\n",
    "                front_set.add(ttmp)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = \"C:/Users/d.baranovska/Downloads/combined_data\"\n",
    "src_dir_front = \"./TPdata/train_data/front\"\n",
    "src_dir_profile = \"./TPdata/train_data/profile\"\n",
    "#relocate_data(src_dir, src_dir_front, src_dir_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frontFile.close()\n",
    "#profileFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontFile = open(\"./TPdata/train_data/front_list.txt\", \"r+\")\n",
    "profileFile = open(\"./TPdata/train_data/profile_list.txt\", \"r+\")\n",
    "front_list = list(map(lambda x: x.strip(), frontFile.readlines()))\n",
    "profile_list = list(map(lambda x: x.strip(),profileFile.readlines()))\n",
    "frontFile.close()\n",
    "profileFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1353\n",
      "1533\n"
     ]
    }
   ],
   "source": [
    "print(len(front_list))\n",
    "print(len(profile_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIC_SIZE = (224, 224)\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, img_list):\n",
    "        super(type(self), self).__init__()\n",
    "        self.img_list = img_list\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    def __getitem__(self, idx):\n",
    "        #229_01_01_200_08_cropped_test.png\n",
    "        img_name = self.img_list[idx]\n",
    "        with Image.open(img_name) as i:\n",
    "            im = i.resize(PIC_SIZE)\n",
    "            our_tensor = transforms.ToTensor()(im)\n",
    "        return our_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f7_shape = [7, 7, 2048]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### second variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self): #, profile, front, train):\n",
    "        super(Generator, self).__init__() \n",
    "#         self.face_model = Resnet50_ft_dag()\n",
    "#         self.feature_p = self.face_model.forward(profile)\n",
    "#         self.feature_f = self.face_model.forward(front)\n",
    "#         self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
    "#         self.is_train = train\n",
    "        \n",
    "        self.conv1 = nn.Sequential(conv2d(2048, 512, kernel_size=1, strides = 1), NORM(512, is_train), nn.ReLU())\n",
    "    \n",
    "        self.res1_1 = res_block(512, 512, norm = NORM, pad_input = 7)\n",
    "        self.res1_2 = res_block(512, 512, norm = NORM, pad_input = 7)\n",
    "        self.dconv2 = nn.Sequential(deconv2d(512, 256, kernel_size=4, strides = 2, pad_input = 7), NORM(256), nn.ReLU())\n",
    "        self.res2 = res_block(256, 256, norm = NORM, pad_input = 14)\n",
    "        self.dconv3 = nn.Sequential(deconv2d(256, 128, kernel_size=4, strides = 2, pad_input = 14) , NORM(128), nn.ReLU())\n",
    "        self.res3 = res_block(128, 128, norm = NORM, pad_input = 28)\n",
    "        self.dconv4 = nn.Sequential(deconv2d(128, 64, kernel_size=4, strides = 2, pad_input = 28), NORM(64), nn.ReLU())\n",
    "        self.res4 = res_block(64, 64, norm = NORM, pad_input = 56)\n",
    "        self.dconv5 = nn.Sequential(deconv2d(64, 32, kernel_size=4, strides = 2, pad_input = 56), NORM(32), nn.ReLU())\n",
    "        self.res5 = res_block(32, 32, norm = NORM, pad_input = 112)\n",
    "        self.dconv6 = nn.Sequential(deconv2d(32, 32, kernel_size=4, strides = 2, pad_input =112),  NORM(32), nn.ReLU())\n",
    "        self.res6 = res_block(32, 32, norm = NORM,  pad_input = 224)\n",
    "        self.gen = nn.Sequential(conv2d(32, 3,  kernel_size=1, strides = 1), nn.Tanh())\n",
    "        \n",
    "    def forward(self, feature):\n",
    "        featt7 = feature[0]\n",
    "        feat7 = self.conv1(featt7)\n",
    "        pool5 = feature[1]\n",
    "        res1_1 = self.res1_1(feat7)\n",
    "        res1_2 = self.res1_2(res1_1)\n",
    "        dconv2 = self.dconv2(res1_2)\n",
    "        res2 = self.res2(dconv2)\n",
    "        dconv3 = self.dconv3(res2)\n",
    "        res3 = self.res3(dconv3)\n",
    "        dconv4 = self.dconv4(res3)\n",
    "        res4 = self.res4(dconv4)\n",
    "        dconv5 = self.dconv5(res4)\n",
    "        res5 = self.res5(dconv5)\n",
    "        dconv6 = self.dconv6(res5)\n",
    "        res6 = self.res6(dconv6)\n",
    "        gen = self.gen(res6)\n",
    "        return (gen + 1)* 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        pad = 1\n",
    "        self.h0_0 = nn.Sequential(conv2d(3, 32, kernel_size=3, strides=2, padding = pad), nn.LeakyReLU())\n",
    "        self.h0_1 = nn.Sequential(conv2d(32, 64, kernel_size=3, strides=2, padding = pad), NORM(64), nn.LeakyReLU())\n",
    "        self.h0_2 = nn.Sequential(conv2d(64, 128, kernel_size=3, strides=2, padding = pad), NORM(128), nn.LeakyReLU())\n",
    "        self.h0_3 = nn.Sequential(conv2d(128, 256, kernel_size=3, strides=2, padding = pad), NORM(256), nn.LeakyReLU())\n",
    "        self.h0_5 = nn.Linear(50176, 1)\n",
    "        \n",
    "        self.h1_0 = nn.Sequential(conv2d(3, 32, kernel_size=3, strides=2, padding = pad), nn.LeakyReLU())\n",
    "        self.h1_1 = nn.Sequential(conv2d(32, 64, kernel_size=3, strides=2, padding = pad), NORM(64), nn.LeakyReLU())\n",
    "        self.h1_2 = nn.Sequential(conv2d(64, 128, kernel_size=3, strides=2, padding = pad), NORM(128), nn.LeakyReLU())\n",
    "        self.h1_3 = nn.Sequential(conv2d(128, 256, kernel_size=3, strides=2, padding = pad), NORM(256), nn.LeakyReLU())\n",
    "        self.h1_4 = nn.Linear(6144, 1)\n",
    "        \n",
    "        self.h2_0 = nn.Sequential(conv2d(3, 32, kernel_size=3, strides=2, padding = pad), nn.LeakyReLU())\n",
    "        self.h2_1 = nn.Sequential(conv2d(32, 64, kernel_size=3, strides=2, padding = pad), NORM(64), nn.LeakyReLU())\n",
    "        self.h2_2 = nn.Sequential(conv2d(64, 128, kernel_size=3, strides=2, padding = pad), NORM(128), nn.LeakyReLU())\n",
    "        self.h2_3 = nn.Sequential(conv2d(128, 256, kernel_size=3, strides=2, padding = pad), NORM(256), nn.LeakyReLU())\n",
    "        #self.h2_4 = nn.Linear(self.h2_3.out_features, 1)\n",
    "        self.h2_4 = nn.Linear(3840, 1)\n",
    "        \n",
    "        self.h3_0 = nn.Sequential(conv2d(3, 32, kernel_size=3, strides=2, padding = pad), nn.LeakyReLU())\n",
    "        self.h3_1 = nn.Sequential(conv2d(32, 64, kernel_size=3, strides=2, padding = pad), NORM(64), nn.LeakyReLU())\n",
    "        self.h3_2 = nn.Sequential(conv2d(64, 128, kernel_size=3, strides=2, padding = pad), NORM(128), nn.LeakyReLU())\n",
    "        self.h3_3 = nn.Sequential(conv2d(128, 256, kernel_size=3, strides=2, padding = pad), NORM(256), nn.LeakyReLU())\n",
    "        self.h3_4 = nn.Linear(2560, 1)\n",
    "        \n",
    "        self.h4_0 = nn.Sequential(conv2d(3, 32, kernel_size=3, strides=2, padding = pad), nn.LeakyReLU())\n",
    "        self.h4_1 = nn.Sequential(conv2d(32, 64, kernel_size=3, strides=2, padding = pad), NORM(64), nn.LeakyReLU())\n",
    "        self.h4_2 = nn.Sequential(conv2d(64, 128, kernel_size=3, strides=2, padding = pad), NORM(128), nn.LeakyReLU())\n",
    "        self.h4_3 = nn.Sequential(conv2d(128, 256, kernel_size=3, strides=2, padding = pad), NORM(256), nn.LeakyReLU())\n",
    "        self.h4_4 = nn.Linear(16384, 1)\n",
    "    \n",
    "    \n",
    "    def forward(self, images):\n",
    "        eyes = images[0:BS,0:CHANNEL,64:100,50:174] #[BS,36,124,CHANNEL])\n",
    "        nose =  images[0:BS,0:CHANNEL,75:140,90:134]#tf.slice(images, [0,75,90,0], [BS,65,44,CHANNEL])\n",
    "        mouth = images[0:BS,0:CHANNEL,140:170,75:149]#tf.slice(images, [0,140,75,0], [BS,30,74,CHANNEL])\n",
    "        face = images[0:BS,0:CHANNEL,64:180,50:174]#[BS,116,124,CHANNEL])\n",
    "        h0_0 = self.h0_0(images)\n",
    "        h0_1 = self.h0_1(h0_0)\n",
    "        h0_2 = self.h0_2(h0_1)\n",
    "        h0_3 = self.h0_3(h0_2)\n",
    "        h0_5 = self.h0_5(torch.reshape(h0_3, [BS, -1]))\n",
    "        \n",
    "        h1_0 = self.h1_0(eyes)\n",
    "        h1_1 = self.h1_1(h1_0)\n",
    "        h1_2 = self.h1_2(h1_1)\n",
    "        h1_3 = self.h1_3(h1_2)\n",
    "        h1_4 = self.h1_4(torch.reshape(h1_3, [BS, -1]))\n",
    "        \n",
    "        h2_0 = self.h2_0(nose)\n",
    "        h2_1 = self.h2_1(h2_0)\n",
    "        h2_2 = self.h2_2(h2_1)\n",
    "        h2_3 = self.h2_3(h2_2)\n",
    "        h2_4 = self.h2_4(torch.reshape(h2_3, [BS, -1]))\n",
    "        \n",
    "        h3_0 = self.h3_0(mouth)\n",
    "        h3_1 = self.h3_1(h3_0)\n",
    "        h3_2 = self.h3_2(h3_1)\n",
    "        h3_3 = self.h3_3(h3_2)\n",
    "        h3_4 = self.h3_4(torch.reshape(h3_3, [BS, -1]))\n",
    "        \n",
    "        h4_0 = self.h4_0(face)\n",
    "        h4_1 = self.h4_1(h4_0)\n",
    "        h4_2 = self.h4_2(h4_1)\n",
    "        h4_3 = self.h4_3(h4_2)\n",
    "        h4_4 = self.h4_4(torch.reshape(h4_3, [BS, -1]))\n",
    "        return torch.cat((h0_5, h1_4, h2_4, h3_4, h4_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#built network\n",
    "z_dim = 224*224\n",
    "mnist_dim = 224*224\n",
    "#mnist_dim = train_dataset_front.train_data.size(1) * train_dataset.train_data.size(2)\n",
    "\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0+cu92\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# optimizer\n",
    "lr = 0.0005 \n",
    "G_optimizer = optim.Adam(G.parameters(), lr)#,betas=(beta1, beta2), weight_decay=0)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr)#, betas=(beta1, beta2), weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Losses = []\n",
    "D_losses, G_losses = [], []\n",
    "D_finalLosses, G_finalLosses = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse(x, dim):\n",
    "    dim = x.dim() + dim if dim < 0 else dim\n",
    "    return x[tuple(slice(None, None) if i != dim\n",
    "             else torch.arange(x.size(i)-1, -1, -1).long()\n",
    "             for i in range(x.dim()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "global a \n",
    "a = 0\n",
    "global epoch_num\n",
    "epoch_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(\"Gen.gen parametrs\", torch.sum(i(*G.parameters())).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(profile, front):\n",
    "    #=======================Train the generator=======================#\n",
    "    G_optimizer.zero_grad()\n",
    "    D_optimizer.zero_grad()\n",
    "    Model.eval()\n",
    "    for p in Model.parameters():\n",
    "        p.require_grad = False\n",
    "    feature_p = Model.forward(profile) # G_enc(x)\n",
    "    feature_f = Model.forward(front) # G_enc(y)\n",
    "#     feature_p = [i for i in feature_p]\n",
    "#     feature_f = [i for i in feature_f]\n",
    "#     for i in feature_p:\n",
    "#         i = i.to(device)\n",
    "#     for i in feature_f:\n",
    "#         i = i.to(device)\n",
    "    gen_p = G.forward(feature_p) # ~x\n",
    "    gen_f = G.forward(feature_f) # ~y\n",
    "    if a % 100 == 0:\n",
    "        print('')\n",
    "        print(a)\n",
    "        print(torch.sum(feature_p[0] - feature_f[0]))\n",
    "        print(torch.sum(gen_p - gen_f))\n",
    "        print(\"gen_p 0 - 1\", torch.sum(gen_p[0] -  gen_p[1]))\n",
    "        print(\"front 0 - 1\", torch.sum(front[0] -  front[1]))\n",
    "        #print(\"Model conv5_3_3x3 sum =\", torch.sum(*Model.conv5_3_3x3.parameters()).item())\n",
    "        #print(\"Gen.dconv6 parameters sum\", torch.sum(*G.dconv6.parameters()).item())\n",
    "        print(\"Gen.gen params sum\", torch.sum(*G.gen.parameters()).item())\n",
    "        #print(\"Dis.h0_5 params sum\", torch.sum(*D.h0_5.parameters()).item())\n",
    "        #save_image(feature_p[0].int(), \"./TPdata/train_data/processed/\" + str(a)+ \"_feature_p\" + \".png\")\n",
    "        save_image(profile, \"I:/train_data/processed/cut_data/epoch\" + str(epoch_num)+\"/\"+ str(a)+\"_profile\" + \".png\")\n",
    "        save_image(gen_f.int(), \"I:/train_data/processed/cut_data/epoch\" + str(epoch_num)+\"/\"+ str(a)+ \"_gen_f_all\" + \".png\")\n",
    "        save_image(gen_p.int(), \"I:/train_data/processed/cut_data/epoch\" +str(epoch_num)+\"/\"+ str(a)+  \"_gen_p_\" + \".png\")\n",
    "    #for i in gen_p: writer.add_image('~x - gen_p', i)\n",
    "    #for i in gen_f: writer.add_image('~y - gen_f', i)\n",
    "#     dr = torch.cat(D(front))\n",
    "#     df1 = torch.cat(D(gen_p))\n",
    "#     df2 = torch.cat(D(gen_f))\n",
    "    \n",
    "    dr = D(front)\n",
    "    df1 = D(gen_p)\n",
    "    df2 = D(gen_f)\n",
    "    feature_gen_p = Model.forward(gen_p) #G_enc(~x)\n",
    "    feature_gen_f = Model.forward(gen_f) #G_enc(~y)\n",
    "    \n",
    "    pool5_p_norm = feature_p[-1]/(torch.norm(feature_p[-1],dim = 1, keepdim = True) + epsilon)\n",
    "    pool5_f_norm = feature_f[-1]/(torch.norm(feature_f[-1],dim = 1, keepdim = True) + epsilon)\n",
    "    \n",
    "    pool5_gen_p_norm = feature_gen_p[-1]/(torch.norm(feature_gen_p[-1],dim = 1, keepdim = True) + epsilon)\n",
    "    pool5_gen_f_norm = feature_gen_f[-1]/(torch.norm(feature_gen_f[-1],dim = 1, keepdim = True) + epsilon)\n",
    "    \n",
    "     # 1. Frontalization Loss: L1-Norm\n",
    "    front_loss = torch.mean(torch.sum(torch.abs(front/255. - gen_f/255.), [1,2,3]))\n",
    "    \n",
    "    # 2. identity perseption loss l2-norm\n",
    "    feature_distance = 0.5*(1 - torch.sum(torch.mul(pool5_p_norm, pool5_gen_p_norm), [1])) + \\\n",
    "                                0.5*(1 - torch.sum(torch.mul(pool5_f_norm, pool5_gen_f_norm), [1]))\n",
    "    feature_loss = torch.mean(feature_distance)\n",
    "    #Losses.append(feature_loss.item())\n",
    "    #.\n",
    "    #trainable var\n",
    "    #all_vars = torch.autograd.Variables()\n",
    "    #vars_gen = Variable(G.parameters, requires_grad= True)\n",
    "    #vars_dis = Variable(D.parameters, requires_grad= True)\n",
    "    #vars_gen = weights_list=[var for var in self.vars_gen if 'kernel' in var.name]\n",
    "    \n",
    "    \n",
    "    # 3. Regulation loss\n",
    "    reg_gen = criterion(gen_p, profile)\n",
    "    reg_dis = criterion(gen_f, front)\n",
    "#     loss = nn.MSELoss(lambda_reg, reduction='mean')\n",
    "#     #reg_gen = loss(weights_list=[var for var in vars_gen]) # if 'kernel' in var.name\n",
    "#     #reg_dis = loss(weights_list=[var for var in vars_dis])# if 'kernel' in var.name\n",
    "#     target = torch.zeros(G.parameters.shape)\n",
    "#     reg_gen = loss(G.parameters, target)\n",
    "#     reg_dis = loss(D.parameters, target)\n",
    "#     G_losses.append(reg_gen)\n",
    "#     D_losses.append(reg_dis)\n",
    "    \n",
    "    \n",
    "    # 4. Adversarial Loss\n",
    "    d_loss = torch.mean(torch.sum(df1)*0.5 + torch.sum(df2)*0.5 - torch.sum(dr)) / 5\n",
    "    g_loss = -torch.mean(torch.sum(df1)*0.5 + torch.sum(df2)*0.5) / 5\n",
    "    #D_losses.append(d_loss.item())\n",
    "    #G_losses.append(g_loss.item())\n",
    "    \n",
    "    # 5. Symmetric Loss - not applied\n",
    "    #mirror_p = reverse(gen_p, dim=[2])\n",
    "    #sym_loss = torch.mean(torch.sum(torchf.abs(mirror_p/225. - gen_p/255.), [1,2,3]))\n",
    "    \n",
    "    # Gradient Penalty #\n",
    "    alpha = torch.rand([front.size(0), 1, 1, 1]).to(device)\n",
    "    inter = (alpha*front + ((1 - alpha) * gen_p )).requires_grad_(True).to(device)\n",
    "    d = D(inter)\n",
    "    fake = Variable(torch.Tensor(BS*5, 1).fill_(1.0), requires_grad=False).to(device)\n",
    "    grad = torch.autograd.grad(d, inter, grad_outputs=fake,\n",
    "    retain_graph=True,\n",
    "    create_graph=False,\n",
    "    only_inputs=True,)[0]\n",
    "    #grad = d.grad\n",
    "    grad = grad.view(grad.size(0), -1)\n",
    "    gradient_penalty = ((grad.norm(2, dim=1)-1)**2).mean()\n",
    "      \n",
    "     # 6. Drift Loss - not applied\n",
    "    #drift_loss = 0\n",
    "    #torch.mean(torch.add(torch.square(df)) + torch.add(torch.square(dr))) / 10\n",
    "    \n",
    "    Gen_loss = lambda_l1*front_loss + lambda_fea*feature_loss +(lambda_gan)*g_loss + reg_gen\n",
    "    Dis_loss = lambda_gan* d_loss + reg_dis#+lambda_gp*gradient_penalty# + torch.tensor(lambda_gp) * gradient_penalty\n",
    "    \n",
    "    #Gen_loss = Gen_loss.to(device)\n",
    "    #Dis_loss = Gen_loss.to(device)\n",
    "    #G_finalLosses.append(Gen_loss.item())\n",
    "    #D_finalLosses.append(Dis_loss.item())\n",
    "#     Gen_loss.backward()\n",
    "#     Dis_loss.backward()\n",
    "    \n",
    "    (Gen_loss + Dis_loss).backward()\n",
    "    \n",
    "\n",
    "    \n",
    "    G_optimizer.step()\n",
    "    D_optimizer.step()\n",
    "#     G_optimizer.zero_grad()\n",
    "#     D_optimizer.zero_grad()\n",
    "    if a < 30:\n",
    "        writer.add_scalar('Gen_loss', Gen_loss , epoch_num*30 + a)\n",
    "        writer.add_scalar('Dis_loss', Dis_loss , epoch_num*30 + a)\n",
    "        writer.add_scalar('front_loss',  front_loss , epoch_num*30 + a)\n",
    "        writer.add_scalar('feature_loss',  feature_loss , epoch_num*30 + a)\n",
    "        writer.add_scalar('g_loss',  g_loss , epoch_num*30 + a)\n",
    "        writer.add_scalar('d_loss',  d_loss , epoch_num*30 + a)\n",
    "        writer.add_scalar(' gradient_penalty',gradient_penalty, epoch_num*30 + a )\n",
    "        writer.add_image('gen_p', gen_p[0], epoch_num*100 + a )\n",
    "        writer.add_image('gen_f', gen_f[0], epoch_num*100 + a)\n",
    "        writer.add_image('front',  front[0], epoch_num*100 + a)\n",
    "        writer.add_image('profile',  profile[0], epoch_num*100 + a)\n",
    "        writer.add_image('gen_p', gen_p[1], epoch_num*100 + a)\n",
    "        writer.add_image('gen_f',  gen_f[1], epoch_num*100 + a)\n",
    "        writer.add_image('front', front[1], epoch_num*100 + a)\n",
    "        writer.add_image('profile',  profile[1], epoch_num*100 + a)\n",
    "        writer.add_image('gen_p', gen_p[2], epoch_num*100 + a)\n",
    "        writer.add_image('gen_f', gen_f[2], epoch_num*100 + a)\n",
    "        writer.add_image('front', front[2], epoch_num*100 + a)\n",
    "        writer.add_image('profile',  profile[2], epoch_num*100 + a)\n",
    "    return Gen_loss,  Dis_loss, front_loss, feature_loss, g_loss, d_loss\n",
    "    \n",
    "#     z = Variable(torch.randn(bs, z_dim).to(device))\n",
    "#     y = Variable(torch.ones(bs, 1).to(device))\n",
    "\n",
    "#     G_output = G(z)\n",
    "#     D_output = D(G_output)\n",
    "#     G_loss = criterion(D_output, y)\n",
    "\n",
    "#     # gradient backprop & optimize ONLY G's parameters\n",
    "#     G_loss.backward()\n",
    "#     G_optimizer.step()\n",
    "        \n",
    "#     return G_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14880864"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in G.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model conv5_3_3x3 sum = -1227.2626953125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224])\n",
      "Model conv5_3_3x3 sum = -1227.2626953125\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand([BS, 3, 224, 224])\n",
    "y = torch.rand([BS, 3, 224, 224])\n",
    "t = Model(x.to(device))\n",
    "x_gen = G(t)\n",
    "#reg_gen = criterion(x_gen, x)\n",
    "#print(reg_gen)\n",
    "print(x_gen.size())\n",
    "print(\"Model conv5_3_3x3 sum =\", torch.sum(*Model.conv5_3_3x3.parameters()).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1]) torch.Size([1]) torch.Size([1]) torch.Size([1]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "x_dis = D(x_gen)\n",
    "print(x_dis[0].size(), x_dis[1].size(), x_dis[2].size(), x_dis[3].size(), x_dis[4].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_front_data = TrainDataset(front_list)\n",
    "Train_profile_data = TrainDataset(profile_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_front = torch.utils.data.DataLoader(dataset=Train_front_data, batch_size=BS, shuffle=True, drop_last=True)\n",
    "train_loader_profile = torch.utils.data.DataLoader(dataset=Train_profile_data, batch_size=BS, shuffle=True, drop_last=True)\n",
    "#test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "d = list(enumerate(train_loader_front))\n",
    "print(d[3][1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'201_01_01_010_08_cropped_test'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(profile_list[0].split('\\\\')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "tensor(439.9107, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-12936.9473, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(49104.8672, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(5005.5645, device='cuda:0')\n",
      "Gen.gen params sum -0.7476621270179749\n",
      "\n",
      "100\n",
      "tensor(384.6089, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.6282, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(0.9084, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-27689.0664, device='cuda:0')\n",
      "Gen.gen params sum -3.113341808319092\n",
      "\n",
      "200\n",
      "tensor(744.8834, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.4250, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(3.0685, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(9198.4980, device='cuda:0')\n",
      "Gen.gen params sum -3.851348876953125\n",
      "\n",
      "300\n",
      "tensor(634.7952, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.1332, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(3.1596, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-12031.3076, device='cuda:0')\n",
      "Gen.gen params sum -4.294421195983887\n",
      "[1/2252]:fea_loss: 0.001,  loss_d: -2596.884, loss_g: -5730.072\n",
      "\n",
      "0\n",
      "tensor(1051.7815, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.3434, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(-3.3101, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(8673.0039, device='cuda:0')\n",
      "Gen.gen params sum -4.425204753875732\n",
      "\n",
      "100\n",
      "tensor(85.6433, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0674, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(0.9297, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-16268.0508, device='cuda:0')\n",
      "Gen.gen params sum -4.708898544311523\n",
      "\n",
      "200\n",
      "tensor(508.5226, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.1739, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(2.4222, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(27024.6484, device='cuda:0')\n",
      "Gen.gen params sum -4.933351516723633\n",
      "\n",
      "300\n",
      "tensor(-29.2021, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.3560, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(5.0780, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(12639.8262, device='cuda:0')\n",
      "Gen.gen params sum -5.119251728057861\n",
      "[2/2252]:fea_loss: 0.000,  loss_d: -7714.711, loss_g: -17444.115\n",
      "\n",
      "0\n",
      "tensor(784.8400, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0659, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(7.6694, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(14664.5137, device='cuda:0')\n",
      "Gen.gen params sum -5.182338237762451\n",
      "\n",
      "100\n",
      "tensor(274.1062, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.1091, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(1.0069, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(1416.8824, device='cuda:0')\n",
      "Gen.gen params sum -5.332561492919922\n",
      "\n",
      "200\n",
      "tensor(474.2372, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0662, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(-0.7992, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-7473.9453, device='cuda:0')\n",
      "Gen.gen params sum -5.46472692489624\n",
      "\n",
      "300\n",
      "tensor(337.3767, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.1324, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(1.4977, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-12380.1455, device='cuda:0')\n",
      "Gen.gen params sum -5.582765102386475\n",
      "[3/2252]:fea_loss: 0.000,  loss_d: -13890.960, loss_g: -35142.168\n",
      "\n",
      "0\n",
      "tensor(450.1734, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.1046, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(1.3160, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-2689.0391, device='cuda:0')\n",
      "Gen.gen params sum -5.624451160430908\n",
      "\n",
      "100\n",
      "tensor(1027.0286, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0529, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(-0.1383, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-8461.2510, device='cuda:0')\n",
      "Gen.gen params sum -5.72754430770874\n",
      "\n",
      "200\n",
      "tensor(137.0307, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0059, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(-0.3698, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-17824.1582, device='cuda:0')\n",
      "Gen.gen params sum -5.821869373321533\n",
      "\n",
      "300\n",
      "tensor(605.2992, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0867, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(1.1374, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(12656.5996, device='cuda:0')\n",
      "Gen.gen params sum -5.909213542938232\n",
      "[4/2252]:fea_loss: 0.000,  loss_d: -21671.264, loss_g: -60277.129\n",
      "\n",
      "0\n",
      "tensor(225.4873, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0855, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(0.0255, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(5195.7334, device='cuda:0')\n",
      "Gen.gen params sum -5.940779209136963\n",
      "\n",
      "100\n",
      "tensor(542.2941, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0942, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(0.4779, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-4757.1060, device='cuda:0')\n",
      "Gen.gen params sum -6.019967555999756\n",
      "\n",
      "200\n",
      "tensor(500.3020, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(0.7278, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(3501.2739, device='cuda:0')\n",
      "Gen.gen params sum -6.094313621520996\n",
      "\n",
      "300\n",
      "tensor(221.3103, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0479, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(0.5640, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-3732.1643, device='cuda:0')\n",
      "Gen.gen params sum -6.164489269256592\n",
      "[5/2252]:fea_loss: 0.000,  loss_d: -30090.434, loss_g: -92027.617\n",
      "\n",
      "0\n",
      "tensor(917.8749, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0655, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(-0.3152, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-2422.5728, device='cuda:0')\n",
      "Gen.gen params sum -6.190243244171143\n",
      "\n",
      "100\n",
      "tensor(644.9107, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0236, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(-0.5746, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-3512.2981, device='cuda:0')\n",
      "Gen.gen params sum -6.25550651550293\n",
      "\n",
      "200\n",
      "tensor(541.7477, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0077, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(0.4943, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-15253.5098, device='cuda:0')\n",
      "Gen.gen params sum -6.317622184753418\n",
      "\n",
      "300\n",
      "tensor(592.8898, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0079, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(0.1297, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-7581.7642, device='cuda:0')\n",
      "Gen.gen params sum -6.376853942871094\n",
      "[6/2252]:fea_loss: 0.000,  loss_d: -35726.441, loss_g: -130470.953\n",
      "\n",
      "0\n",
      "tensor(-57.2589, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0557, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(0.3282, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-8355.9961, device='cuda:0')\n",
      "Gen.gen params sum -6.398727893829346\n",
      "\n",
      "100\n",
      "tensor(535.2936, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0741, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(0.0054, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-9376.9014, device='cuda:0')\n",
      "Gen.gen params sum -6.454529285430908\n",
      "\n",
      "200\n",
      "tensor(814.7589, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0651, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(-0.1590, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(11894.8779, device='cuda:0')\n",
      "Gen.gen params sum -6.507948398590088\n",
      "\n",
      "300\n",
      "tensor(338.9877, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0286, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(0.1789, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-987.4393, device='cuda:0')\n",
      "Gen.gen params sum -6.559250831604004\n",
      "[7/2252]:fea_loss: 0.000,  loss_d: -45081.430, loss_g: -173405.500\n",
      "\n",
      "0\n",
      "tensor(546.0850, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0909, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(-0.1880, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-20804.2832, device='cuda:0')\n",
      "Gen.gen params sum -6.578151226043701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100\n",
      "tensor(1429.8650, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(0.5238, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-14059.0469, device='cuda:0')\n",
      "Gen.gen params sum -6.626132011413574\n",
      "\n",
      "200\n",
      "tensor(38.2838, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0204, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(0.6988, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(37250.6328, device='cuda:0')\n",
      "Gen.gen params sum -6.6720991134643555\n",
      "\n",
      "300\n",
      "tensor(773.3919, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0334, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(-1.1971, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(15995.6992, device='cuda:0')\n",
      "Gen.gen params sum -6.716124534606934\n",
      "[8/2252]:fea_loss: 0.000,  loss_d: -56167.371, loss_g: -225105.562\n",
      "\n",
      "0\n",
      "tensor(347.5210, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0139, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(-0.4689, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-3769.2122, device='cuda:0')\n",
      "Gen.gen params sum -6.732308387756348\n",
      "\n",
      "100\n",
      "tensor(352.0637, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0091, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(-2.1367, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-4063.9888, device='cuda:0')\n",
      "Gen.gen params sum -6.773405075073242\n",
      "\n",
      "200\n",
      "tensor(616.9847, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0195, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(-0.9076, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(12848.8203, device='cuda:0')\n",
      "Gen.gen params sum -6.813002586364746\n",
      "\n",
      "300\n",
      "tensor(691.5754, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0191, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(1.6290, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(7266.5884, device='cuda:0')\n",
      "Gen.gen params sum -6.850893974304199\n",
      "[9/2252]:fea_loss: 0.000,  loss_d: -63131.355, loss_g: -282897.062\n",
      "\n",
      "0\n",
      "tensor(1007.2262, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0148, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(0.0355, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(21454.8770, device='cuda:0')\n",
      "Gen.gen params sum -6.864919662475586\n",
      "\n",
      "100\n",
      "tensor(302.7677, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0154, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(-2.1428, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-11582.0703, device='cuda:0')\n",
      "Gen.gen params sum -6.900938510894775\n",
      "\n",
      "200\n",
      "tensor(703.1159, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(0.4066, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(6474.0156, device='cuda:0')\n",
      "Gen.gen params sum -6.93538236618042\n",
      "\n",
      "300\n",
      "tensor(593.2625, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(-2.8756, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(5828.0273, device='cuda:0')\n",
      "Gen.gen params sum -6.968258857727051\n",
      "[10/2252]:fea_loss: 0.000,  loss_d: -68147.867, loss_g: -345742.188\n",
      "\n",
      "0\n",
      "tensor(712.4346, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0562, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(1.7552, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-18635.6660, device='cuda:0')\n",
      "Gen.gen params sum -6.980367660522461\n",
      "\n",
      "100\n",
      "tensor(-65.0111, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0177, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(-3.9157, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(2524.6707, device='cuda:0')\n",
      "Gen.gen params sum -7.011194229125977\n",
      "\n",
      "200\n",
      "tensor(524.3588, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0695, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(2.4641, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-5950.4160, device='cuda:0')\n",
      "Gen.gen params sum -7.040998458862305\n",
      "\n",
      "300\n",
      "tensor(777.2100, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0532, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(1.1526, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(35947.4141, device='cuda:0')\n",
      "Gen.gen params sum -7.068930149078369\n",
      "[11/2252]:fea_loss: 0.000,  loss_d: -80393.492, loss_g: -412499.000\n",
      "\n",
      "0\n",
      "tensor(382.2316, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-0.0216, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "gen_p 0 - 1 tensor(-0.5237, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "front 0 - 1 tensor(-34262.0820, device='cuda:0')\n",
      "Gen.gen params sum -7.079132080078125\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 2252\n",
    "for epoch in range(1, n_epoch+1):\n",
    "    if not os.path.isdir(\"I:/train_data/processed/cut_data/epoch\" + str(epoch_num)):\n",
    "        os.mkdir( \"I:/train_data/processed/cut_data/epoch\" + str(epoch_num))\n",
    "    D_losses, G_losses = [], []\n",
    "    d = list(enumerate(train_loader_front))\n",
    "    for j in range(len(d)):\n",
    "        d[j] = (j, torch.autograd.Variable(d[j][1].to(device), requires_grad = False))\n",
    "    for batch_idx, x in enumerate(train_loader_profile):\n",
    "        for k in range(len(x)):\n",
    "            x[k] = torch.autograd.Variable(x[k].to(device), requires_grad = False)\n",
    "        x = x.to(device)\n",
    "        if batch_idx >= len(d):\n",
    "            break\n",
    "        Gen_loss,  Dis_loss, front_loss, feature_loss, g_loss, d_loss = loss(x, d[batch_idx][1])\n",
    "        D_losses.append(Gen_loss)\n",
    "        G_losses.append(Dis_loss)\n",
    "        \n",
    "        a += 1\n",
    "    a = 0\n",
    "    epoch_num += 1 \n",
    "        #print(list(map(lambda x: x.item(), [Gen_loss,  Dis_loss, front_loss, feature_loss, g_loss, d_loss])))\n",
    "\n",
    "    \n",
    "    print('[%d/%d]:fea_loss: %.3f,  loss_d: %.3f, loss_g: %.3f' % \n",
    "            ((epoch), n_epoch,feature_loss, Dis_loss, Gen_loss))\n",
    "    #writer.add_histogram('Gen_losses', np.array(G_losses))\n",
    "    #writer.add_histogram('Dis_losses', np.array(D_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
